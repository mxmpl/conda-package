//////////////////////////////////////////////////////////////////////
// This file was automatically generated by CLIF to run under Python 3
// Version 0.3
//////////////////////////////////////////////////////////////////////
// source: /pykaldi/kaldi/ivector/logistic-regression.clif

#include <Python.h>
#include "clif/python/ptr_util.h"
#include "clif/python/optional.h"
#include "clif/python/types.h"
#include "matrix/kaldi-vector-clifwrap.h"
#include "matrix/kaldi-matrix-clifwrap.h"
#include "itf/options-itf-clifwrap.h"
#include "base/iostream-clifwrap.h"
#include "logistic-regression-clifwrap.h"
#include "clif/python/stltypes.h"
#include "clif/python/slots.h"

namespace __logistic__regression_clifwrap {
using namespace clif;

#define _0 py::postconv::PASS
#define _1 UnicodeFromBytes
#define _2 UnicodeFromBytes


namespace pyLogisticRegressionConfig {

struct wrapper {
  PyObject_HEAD
  ::clif::Instance<::kaldi::LogisticRegressionConfig> cpp;
};
static ::kaldi::LogisticRegressionConfig* ThisPtr(PyObject*);

static PyObject* get_max_steps(PyObject* self, void* xdata) {
  auto cpp = ThisPtr(self); if (!cpp) return nullptr;
  return Clif_PyObjFrom(cpp->max_steps, {});
}

static int set_max_steps(PyObject* self, PyObject* value, void* xdata) {
  if (value == nullptr) {
    PyErr_SetString(PyExc_TypeError, "Cannot delete the max_steps attribute");
    return -1;
  }
  auto cpp = ThisPtr(self); if (!cpp) return -1;
  if (Clif_PyObjAs(value, &cpp->max_steps)) return 0;
  PyObject* s = PyObject_Repr(value);
  PyErr_Format(PyExc_ValueError, "%s is not valid for max_steps:int", s? PyUnicode_AsUTF8(s): "input");
  Py_XDECREF(s);
  return -1;
}

static PyObject* get_mix_up(PyObject* self, void* xdata) {
  auto cpp = ThisPtr(self); if (!cpp) return nullptr;
  return Clif_PyObjFrom(cpp->mix_up, {});
}

static int set_mix_up(PyObject* self, PyObject* value, void* xdata) {
  if (value == nullptr) {
    PyErr_SetString(PyExc_TypeError, "Cannot delete the mix_up attribute");
    return -1;
  }
  auto cpp = ThisPtr(self); if (!cpp) return -1;
  if (Clif_PyObjAs(value, &cpp->mix_up)) return 0;
  PyObject* s = PyObject_Repr(value);
  PyErr_Format(PyExc_ValueError, "%s is not valid for mix_up:int", s? PyUnicode_AsUTF8(s): "input");
  Py_XDECREF(s);
  return -1;
}

static PyObject* get_normalizer(PyObject* self, void* xdata) {
  auto cpp = ThisPtr(self); if (!cpp) return nullptr;
  return Clif_PyObjFrom(cpp->normalizer, {});
}

static int set_normalizer(PyObject* self, PyObject* value, void* xdata) {
  if (value == nullptr) {
    PyErr_SetString(PyExc_TypeError, "Cannot delete the normalizer attribute");
    return -1;
  }
  auto cpp = ThisPtr(self); if (!cpp) return -1;
  if (Clif_PyObjAs(value, &cpp->normalizer)) return 0;
  PyObject* s = PyObject_Repr(value);
  PyErr_Format(PyExc_ValueError, "%s is not valid for normalizer:float", s? PyUnicode_AsUTF8(s): "input");
  Py_XDECREF(s);
  return -1;
}

static PyObject* get_power(PyObject* self, void* xdata) {
  auto cpp = ThisPtr(self); if (!cpp) return nullptr;
  return Clif_PyObjFrom(cpp->power, {});
}

static int set_power(PyObject* self, PyObject* value, void* xdata) {
  if (value == nullptr) {
    PyErr_SetString(PyExc_TypeError, "Cannot delete the power attribute");
    return -1;
  }
  auto cpp = ThisPtr(self); if (!cpp) return -1;
  if (Clif_PyObjAs(value, &cpp->power)) return 0;
  PyObject* s = PyObject_Repr(value);
  PyErr_Format(PyExc_ValueError, "%s is not valid for power:float", s? PyUnicode_AsUTF8(s): "input");
  Py_XDECREF(s);
  return -1;
}

// register(opts:OptionsItf)
static PyObject* wrapRegister_as_register(PyObject* self, PyObject* args, PyObject* kw) {
  PyObject* a[1];
  char* names[] = {
      C("opts"),
      nullptr
  };
  if (!PyArg_ParseTupleAndKeywords(args, kw, "O:register", names, &a[0])) return nullptr;
  ::kaldi::OptionsItf * arg1;
  if (!Clif_PyObjAs(a[0], &arg1)) return ArgError("register", names[0], "::kaldi::OptionsItf *", a[0]);
  // Call actual C++ method.
  ::kaldi::LogisticRegressionConfig* c = ThisPtr(self);
  if (!c) return nullptr;
  Py_INCREF(args);
  Py_XINCREF(kw);
  PyThreadState* _save;
  Py_UNBLOCK_THREADS
  PyObject* err_type = nullptr;
  string err_msg{"C++ exception"};
  try {
    c->Register(arg1);
  } catch(const std::exception& e) {
    err_type = PyExc_RuntimeError;
    err_msg += string(": ") + e.what();
  } catch (...) {
    err_type = PyExc_RuntimeError;
  }
  Py_BLOCK_THREADS
  Py_DECREF(args);
  Py_XDECREF(kw);
  if (err_type) {
    PyErr_SetString(err_type, err_msg.c_str());
    return nullptr;
  }
  Py_RETURN_NONE;
}

static PyGetSetDef Properties[] = {
  {C("max_steps"), get_max_steps, set_max_steps, C("C++ ::int32 LogisticRegressionConfig.max_steps")},
  {C("mix_up"), get_mix_up, set_mix_up, C("C++ ::int32 LogisticRegressionConfig.mix_up")},
  {C("normalizer"), get_normalizer, set_normalizer, C("C++ double LogisticRegressionConfig.normalizer")},
  {C("power"), get_power, set_power, C("C++ double LogisticRegressionConfig.power")},
  {}
};

static PyMethodDef Methods[] = {
  {C("register"), (PyCFunction)wrapRegister_as_register, METH_VARARGS | METH_KEYWORDS, C("register(opts:OptionsItf)\n  Calls C++ function\n  void ::kaldi::LogisticRegressionConfig::Register(::kaldi::OptionsItf *)")},
  {}
};

// LogisticRegressionConfig __init__
static int _ctor(PyObject* self, PyObject* args, PyObject* kw);

// LogisticRegressionConfig __new__
static PyObject* _new(PyTypeObject* type, Py_ssize_t nitems);

// LogisticRegressionConfig __del__
static void _del(void* self) {
  delete reinterpret_cast<wrapper*>(self);
}

PyTypeObject wrapper_Type = {
  PyVarObject_HEAD_INIT(&PyType_Type, 0)
  "_logistic_regression.LogisticRegressionConfig", // tp_name
  sizeof(wrapper),                     // tp_basicsize
  0,                                   // tp_itemsize
  nullptr,                             // tp_dealloc
  nullptr,                             // tp_print
  nullptr,                             // tp_getattr
  nullptr,                             // tp_setattr
  nullptr,                             // tp_compare
  nullptr,                             // tp_repr
  nullptr,                             // tp_as_number
  nullptr,                             // tp_as_sequence
  nullptr,                             // tp_as_mapping
  nullptr,                             // tp_hash
  nullptr,                             // tp_call
  nullptr,                             // tp_str
  nullptr,                             // tp_getattro
  nullptr,                             // tp_setattro
  nullptr,                             // tp_as_buffer
  Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE, // tp_flags
  "CLIF wrapper for ::kaldi::LogisticRegressionConfig", // tp_doc
  nullptr,                             // tp_traverse
  nullptr,                             // tp_clear
  nullptr,                             // tp_richcompare
  0,                                   // tp_weaklistoffset
  nullptr,                             // tp_iter
  nullptr,                             // tp_iternext
  Methods,                             // tp_methods
  nullptr,                             // tp_members
  Properties,                          // tp_getset
  nullptr,                             // tp_base
  nullptr,                             // tp_dict
  nullptr,                             // tp_descr_get
  nullptr,                             // tp_descr_set
  0,                                   // tp_dictoffset
  _ctor,                               // tp_init
  _new,                                // tp_alloc
  PyType_GenericNew,                   // tp_new
  _del,                                // tp_free
  nullptr,                             // tp_is_gc
  nullptr,                             // tp_bases
  nullptr,                             // tp_mro
  nullptr,                             // tp_cache
  nullptr,                             // tp_subclasses
  nullptr,                             // tp_weaklist
  nullptr,                             // tp_del
  0,                                   // tp_version_tag
};

static int _ctor(PyObject* self, PyObject* args, PyObject* kw) {
  if ((args && PyTuple_GET_SIZE(args) != 0) || (kw && PyDict_Size(kw) != 0)) {
    PyErr_SetString(PyExc_TypeError, "LogisticRegressionConfig takes no arguments");
    return -1;
  }
  reinterpret_cast<wrapper*>(self)->cpp = ::clif::MakeShared<::kaldi::LogisticRegressionConfig>();
  return 0;
}

static PyObject* _new(PyTypeObject* type, Py_ssize_t nitems) {
  assert(nitems == 0);
  PyObject* self = reinterpret_cast<PyObject*>(new wrapper);
  return PyObject_Init(self, &wrapper_Type);
}

static ::kaldi::LogisticRegressionConfig* ThisPtr(PyObject* py) {
  if (Py_TYPE(py) == &wrapper_Type) {
    return ::clif::python::Get(reinterpret_cast<wrapper*>(py)->cpp);
  }
  PyObject* base = PyObject_CallMethod(py, C("as_kaldi_LogisticRegressionConfig"), nullptr);
  if (base) {
    if (PyCapsule_CheckExact(base)) {
      void* p = PyCapsule_GetPointer(base, C("::kaldi::LogisticRegressionConfig"));
      if (!PyErr_Occurred()) {
        ::kaldi::LogisticRegressionConfig* c = static_cast<::kaldi::LogisticRegressionConfig*>(p);
        Py_DECREF(base);
        return c;
      }
    }
    Py_DECREF(base);
  }
  if (PyObject_IsInstance(py, reinterpret_cast<PyObject*>(&wrapper_Type))) {
    if (!base) {
      PyErr_Clear();
      return ::clif::python::Get(reinterpret_cast<wrapper*>(py)->cpp);
    }
    PyErr_Format(PyExc_ValueError, "can't convert %s %s to ::kaldi::LogisticRegressionConfig*", ClassName(py), ClassType(py));
  } else {
    PyErr_Format(PyExc_TypeError, "expecting %s instance, got %s %s", wrapper_Type.tp_name, ClassName(py), ClassType(py));
  }
  return nullptr;
}
}  // namespace pyLogisticRegressionConfig

namespace pyLogisticRegression {

struct wrapper {
  PyObject_HEAD
  ::clif::Instance<::kaldi::LogisticRegression> cpp;
};
static ::kaldi::LogisticRegression* ThisPtr(PyObject*);

// train(xs:Matrix, ys:list<int>, conf:LogisticRegressionConfig)
static PyObject* wrapTrain_as_train(PyObject* self, PyObject* args, PyObject* kw) {
  PyObject* a[3];
  char* names[] = {
      C("xs"),
      C("ys"),
      C("conf"),
      nullptr
  };
  if (!PyArg_ParseTupleAndKeywords(args, kw, "OOO:train", names, &a[0], &a[1], &a[2])) return nullptr;
  ::kaldi::Matrix<float>* arg1;
  if (!Clif_PyObjAs(a[0], &arg1)) return ArgError("train", names[0], "::kaldi::Matrix<float>", a[0]);
  ::std::vector< ::int32> arg2;
  if (!Clif_PyObjAs(a[1], &arg2)) return ArgError("train", names[1], "::std::vector< ::int32>", a[1]);
  ::kaldi::LogisticRegressionConfig* arg3;
  if (!Clif_PyObjAs(a[2], &arg3)) return ArgError("train", names[2], "::kaldi::LogisticRegressionConfig", a[2]);
  // Call actual C++ method.
  ::kaldi::LogisticRegression* c = ThisPtr(self);
  if (!c) return nullptr;
  Py_INCREF(args);
  Py_XINCREF(kw);
  PyThreadState* _save;
  Py_UNBLOCK_THREADS
  PyObject* err_type = nullptr;
  string err_msg{"C++ exception"};
  try {
    c->Train(*arg1, std::move(arg2), *arg3);
  } catch(const std::exception& e) {
    err_type = PyExc_RuntimeError;
    err_msg += string(": ") + e.what();
  } catch (...) {
    err_type = PyExc_RuntimeError;
  }
  Py_BLOCK_THREADS
  Py_DECREF(args);
  Py_XDECREF(kw);
  if (err_type) {
    PyErr_SetString(err_type, err_msg.c_str());
    return nullptr;
  }
  Py_RETURN_NONE;
}

// get_log_posteriors_matrix(xs:Matrix) -> Matrix
static PyObject* wrapGetLogPosteriors_as_get_log_posteriors_matrix(PyObject* self, PyObject* args, PyObject* kw) {
  PyObject* a[1];
  char* names[] = {
      C("xs"),
      nullptr
  };
  if (!PyArg_ParseTupleAndKeywords(args, kw, "O:get_log_posteriors_matrix", names, &a[0])) return nullptr;
  ::kaldi::Matrix<float>* arg1;
  if (!Clif_PyObjAs(a[0], &arg1)) return ArgError("get_log_posteriors_matrix", names[0], "::kaldi::Matrix<float>", a[0]);
  ::kaldi::Matrix<float> ret0{};
  // Call actual C++ method.
  ::kaldi::LogisticRegression* c = ThisPtr(self);
  if (!c) return nullptr;
  Py_INCREF(args);
  Py_XINCREF(kw);
  PyThreadState* _save;
  Py_UNBLOCK_THREADS
  PyObject* err_type = nullptr;
  string err_msg{"C++ exception"};
  try {
    c->GetLogPosteriors(*arg1, &ret0);
  } catch(const std::exception& e) {
    err_type = PyExc_RuntimeError;
    err_msg += string(": ") + e.what();
  } catch (...) {
    err_type = PyExc_RuntimeError;
  }
  Py_BLOCK_THREADS
  Py_DECREF(args);
  Py_XDECREF(kw);
  if (err_type) {
    PyErr_SetString(err_type, err_msg.c_str());
    return nullptr;
  }
  return Clif_PyObjFrom(std::move(ret0), {});
}

// get_log_posteriors_vector(x:Vector) -> Vector
static PyObject* wrapGetLogPosteriors_as_get_log_posteriors_vector(PyObject* self, PyObject* args, PyObject* kw) {
  PyObject* a[1];
  char* names[] = {
      C("x"),
      nullptr
  };
  if (!PyArg_ParseTupleAndKeywords(args, kw, "O:get_log_posteriors_vector", names, &a[0])) return nullptr;
  ::kaldi::Vector<float>* arg1;
  if (!Clif_PyObjAs(a[0], &arg1)) return ArgError("get_log_posteriors_vector", names[0], "::kaldi::Vector<float>", a[0]);
  ::kaldi::Vector<float> ret0{};
  // Call actual C++ method.
  ::kaldi::LogisticRegression* c = ThisPtr(self);
  if (!c) return nullptr;
  Py_INCREF(args);
  Py_XINCREF(kw);
  PyThreadState* _save;
  Py_UNBLOCK_THREADS
  PyObject* err_type = nullptr;
  string err_msg{"C++ exception"};
  try {
    c->GetLogPosteriors(*arg1, &ret0);
  } catch(const std::exception& e) {
    err_type = PyExc_RuntimeError;
    err_msg += string(": ") + e.what();
  } catch (...) {
    err_type = PyExc_RuntimeError;
  }
  Py_BLOCK_THREADS
  Py_DECREF(args);
  Py_XDECREF(kw);
  if (err_type) {
    PyErr_SetString(err_type, err_msg.c_str());
    return nullptr;
  }
  return Clif_PyObjFrom(std::move(ret0), {});
}

// write(os:ostream, binary:bool)
static PyObject* wrapWrite_as_write(PyObject* self, PyObject* args, PyObject* kw) {
  PyObject* a[2];
  char* names[] = {
      C("os"),
      C("binary"),
      nullptr
  };
  if (!PyArg_ParseTupleAndKeywords(args, kw, "OO:write", names, &a[0], &a[1])) return nullptr;
  ::std::basic_ostream<char, ::std::char_traits<char> >* arg1;
  if (!Clif_PyObjAs(a[0], &arg1)) return ArgError("write", names[0], "::std::basic_ostream<char, ::std::char_traits<char> >", a[0]);
  bool arg2;
  if (!Clif_PyObjAs(a[1], &arg2)) return ArgError("write", names[1], "bool", a[1]);
  // Call actual C++ method.
  ::kaldi::LogisticRegression* c = ThisPtr(self);
  if (!c) return nullptr;
  Py_INCREF(args);
  Py_XINCREF(kw);
  PyThreadState* _save;
  Py_UNBLOCK_THREADS
  PyObject* err_type = nullptr;
  string err_msg{"C++ exception"};
  try {
    c->Write(*arg1, std::move(arg2));
  } catch(const std::exception& e) {
    err_type = PyExc_RuntimeError;
    err_msg += string(": ") + e.what();
  } catch (...) {
    err_type = PyExc_RuntimeError;
  }
  Py_BLOCK_THREADS
  Py_DECREF(args);
  Py_XDECREF(kw);
  if (err_type) {
    PyErr_SetString(err_type, err_msg.c_str());
    return nullptr;
  }
  Py_RETURN_NONE;
}

// read(os:istream, binary:bool)
static PyObject* wrapRead_as_read(PyObject* self, PyObject* args, PyObject* kw) {
  PyObject* a[2];
  char* names[] = {
      C("os"),
      C("binary"),
      nullptr
  };
  if (!PyArg_ParseTupleAndKeywords(args, kw, "OO:read", names, &a[0], &a[1])) return nullptr;
  ::std::basic_istream<char, ::std::char_traits<char> >* arg1;
  if (!Clif_PyObjAs(a[0], &arg1)) return ArgError("read", names[0], "::std::basic_istream<char, ::std::char_traits<char> >", a[0]);
  bool arg2;
  if (!Clif_PyObjAs(a[1], &arg2)) return ArgError("read", names[1], "bool", a[1]);
  // Call actual C++ method.
  ::kaldi::LogisticRegression* c = ThisPtr(self);
  if (!c) return nullptr;
  Py_INCREF(args);
  Py_XINCREF(kw);
  PyThreadState* _save;
  Py_UNBLOCK_THREADS
  PyObject* err_type = nullptr;
  string err_msg{"C++ exception"};
  try {
    c->Read(*arg1, std::move(arg2));
  } catch(const std::exception& e) {
    err_type = PyExc_RuntimeError;
    err_msg += string(": ") + e.what();
  } catch (...) {
    err_type = PyExc_RuntimeError;
  }
  Py_BLOCK_THREADS
  Py_DECREF(args);
  Py_XDECREF(kw);
  if (err_type) {
    PyErr_SetString(err_type, err_msg.c_str());
    return nullptr;
  }
  Py_RETURN_NONE;
}

// scale_priors(prior_scales:Vector)
static PyObject* wrapScalePriors_as_scale_priors(PyObject* self, PyObject* args, PyObject* kw) {
  PyObject* a[1];
  char* names[] = {
      C("prior_scales"),
      nullptr
  };
  if (!PyArg_ParseTupleAndKeywords(args, kw, "O:scale_priors", names, &a[0])) return nullptr;
  ::kaldi::Vector<float>* arg1;
  if (!Clif_PyObjAs(a[0], &arg1)) return ArgError("scale_priors", names[0], "::kaldi::Vector<float>", a[0]);
  // Call actual C++ method.
  ::kaldi::LogisticRegression* c = ThisPtr(self);
  if (!c) return nullptr;
  Py_INCREF(args);
  Py_XINCREF(kw);
  PyThreadState* _save;
  Py_UNBLOCK_THREADS
  PyObject* err_type = nullptr;
  string err_msg{"C++ exception"};
  try {
    c->ScalePriors(*arg1);
  } catch(const std::exception& e) {
    err_type = PyExc_RuntimeError;
    err_msg += string(": ") + e.what();
  } catch (...) {
    err_type = PyExc_RuntimeError;
  }
  Py_BLOCK_THREADS
  Py_DECREF(args);
  Py_XDECREF(kw);
  if (err_type) {
    PyErr_SetString(err_type, err_msg.c_str());
    return nullptr;
  }
  Py_RETURN_NONE;
}

static PyMethodDef Methods[] = {
  {C("train"), (PyCFunction)wrapTrain_as_train, METH_VARARGS | METH_KEYWORDS, C("train(xs:Matrix, ys:list<int>, conf:LogisticRegressionConfig)\n\nxs and ys are the trainning data. Each row of xs is a vector corresponding to the class label in the same row of ys.")},
  {C("get_log_posteriors_matrix"), (PyCFunction)wrapGetLogPosteriors_as_get_log_posteriors_matrix, METH_VARARGS | METH_KEYWORDS, C("get_log_posteriors_matrix(xs:Matrix) -> Matrix\n\nCalculates the log posterior of the class label given the input xs")},
  {C("get_log_posteriors_vector"), (PyCFunction)wrapGetLogPosteriors_as_get_log_posteriors_vector, METH_VARARGS | METH_KEYWORDS, C("get_log_posteriors_vector(x:Vector) -> Vector\n\nCalculates the log posterior of the class label given the input x.")},
  {C("write"), (PyCFunction)wrapWrite_as_write, METH_VARARGS | METH_KEYWORDS, C("write(os:ostream, binary:bool)\n  Calls C++ function\n  void ::kaldi::LogisticRegression::Write(::std::basic_ostream<char, ::std::char_traits<char> >, bool)")},
  {C("read"), (PyCFunction)wrapRead_as_read, METH_VARARGS | METH_KEYWORDS, C("read(os:istream, binary:bool)\n  Calls C++ function\n  void ::kaldi::LogisticRegression::Read(::std::basic_istream<char, ::std::char_traits<char> >, bool)")},
  {C("scale_priors"), (PyCFunction)wrapScalePriors_as_scale_priors, METH_VARARGS | METH_KEYWORDS, C("scale_priors(prior_scales:Vector)\n  Calls C++ function\n  void ::kaldi::LogisticRegression::ScalePriors(::kaldi::Vector<float>)")},
  {}
};

// LogisticRegression __init__
static int _ctor(PyObject* self, PyObject* args, PyObject* kw);

// LogisticRegression __new__
static PyObject* _new(PyTypeObject* type, Py_ssize_t nitems);

// LogisticRegression __del__
static void _dtor(PyObject* self) {
  Py_BEGIN_ALLOW_THREADS
  reinterpret_cast<wrapper*>(self)->cpp.Destruct();
  Py_END_ALLOW_THREADS
  Py_TYPE(self)->tp_free(self);
}
static void _del(void* self) {
  delete reinterpret_cast<wrapper*>(self);
}

PyTypeObject wrapper_Type = {
  PyVarObject_HEAD_INIT(&PyType_Type, 0)
  "_logistic_regression.LogisticRegression", // tp_name
  sizeof(wrapper),                     // tp_basicsize
  0,                                   // tp_itemsize
  _dtor,                               // tp_dealloc
  nullptr,                             // tp_print
  nullptr,                             // tp_getattr
  nullptr,                             // tp_setattr
  nullptr,                             // tp_compare
  nullptr,                             // tp_repr
  nullptr,                             // tp_as_number
  nullptr,                             // tp_as_sequence
  nullptr,                             // tp_as_mapping
  nullptr,                             // tp_hash
  nullptr,                             // tp_call
  nullptr,                             // tp_str
  nullptr,                             // tp_getattro
  nullptr,                             // tp_setattro
  nullptr,                             // tp_as_buffer
  Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE, // tp_flags
  "CLIF wrapper for ::kaldi::LogisticRegression", // tp_doc
  nullptr,                             // tp_traverse
  nullptr,                             // tp_clear
  nullptr,                             // tp_richcompare
  0,                                   // tp_weaklistoffset
  nullptr,                             // tp_iter
  nullptr,                             // tp_iternext
  Methods,                             // tp_methods
  nullptr,                             // tp_members
  nullptr,                             // tp_getset
  nullptr,                             // tp_base
  nullptr,                             // tp_dict
  nullptr,                             // tp_descr_get
  nullptr,                             // tp_descr_set
  0,                                   // tp_dictoffset
  _ctor,                               // tp_init
  _new,                                // tp_alloc
  PyType_GenericNew,                   // tp_new
  _del,                                // tp_free
  nullptr,                             // tp_is_gc
  nullptr,                             // tp_bases
  nullptr,                             // tp_mro
  nullptr,                             // tp_cache
  nullptr,                             // tp_subclasses
  nullptr,                             // tp_weaklist
  nullptr,                             // tp_del
  0,                                   // tp_version_tag
};

static int _ctor(PyObject* self, PyObject* args, PyObject* kw) {
  if ((args && PyTuple_GET_SIZE(args) != 0) || (kw && PyDict_Size(kw) != 0)) {
    PyErr_SetString(PyExc_TypeError, "LogisticRegression takes no arguments");
    return -1;
  }
  reinterpret_cast<wrapper*>(self)->cpp = ::clif::MakeShared<::kaldi::LogisticRegression>();
  return 0;
}

static PyObject* _new(PyTypeObject* type, Py_ssize_t nitems) {
  assert(nitems == 0);
  PyObject* self = reinterpret_cast<PyObject*>(new wrapper);
  return PyObject_Init(self, &wrapper_Type);
}

static ::kaldi::LogisticRegression* ThisPtr(PyObject* py) {
  if (Py_TYPE(py) == &wrapper_Type) {
    return ::clif::python::Get(reinterpret_cast<wrapper*>(py)->cpp);
  }
  PyObject* base = PyObject_CallMethod(py, C("as_kaldi_LogisticRegression"), nullptr);
  if (base) {
    if (PyCapsule_CheckExact(base)) {
      void* p = PyCapsule_GetPointer(base, C("::kaldi::LogisticRegression"));
      if (!PyErr_Occurred()) {
        ::kaldi::LogisticRegression* c = static_cast<::kaldi::LogisticRegression*>(p);
        Py_DECREF(base);
        return c;
      }
    }
    Py_DECREF(base);
  }
  if (PyObject_IsInstance(py, reinterpret_cast<PyObject*>(&wrapper_Type))) {
    if (!base) {
      PyErr_Clear();
      return ::clif::python::Get(reinterpret_cast<wrapper*>(py)->cpp);
    }
    PyErr_Format(PyExc_ValueError, "can't convert %s %s to ::kaldi::LogisticRegression*", ClassName(py), ClassType(py));
  } else {
    PyErr_Format(PyExc_TypeError, "expecting %s instance, got %s %s", wrapper_Type.tp_name, ClassName(py), ClassType(py));
  }
  return nullptr;
}
}  // namespace pyLogisticRegression


// Initialize module

bool Ready() {
  if (PyType_Ready(&pyLogisticRegressionConfig::wrapper_Type) < 0) return false;
  Py_INCREF(&pyLogisticRegressionConfig::wrapper_Type);  // For PyModule_AddObject to steal.
  if (PyType_Ready(&pyLogisticRegression::wrapper_Type) < 0) return false;
  Py_INCREF(&pyLogisticRegression::wrapper_Type);  // For PyModule_AddObject to steal.
  return true;
}

static struct PyModuleDef Module = {
  PyModuleDef_HEAD_INIT,
  "_logistic_regression",  // module name
  "CLIF-generated module for ivector/logistic-regression.h", // module doc
  -1,  // module keeps state in global variables
  nullptr
};

PyObject* Init() {
  PyObject* module = PyModule_Create(&Module);
  if (!module) return nullptr;
  if (PyObject* m = PyImport_ImportModule("_kaldi_vector")) Py_DECREF(m);
  else goto err;
  if (PyObject* m = PyImport_ImportModule("_kaldi_matrix")) Py_DECREF(m);
  else goto err;
  if (PyObject* m = PyImport_ImportModule("_options_itf")) Py_DECREF(m);
  else goto err;
  if (PyObject* m = PyImport_ImportModule("_iostream")) Py_DECREF(m);
  else goto err;
  PyEval_InitThreads();
  if (PyModule_AddObject(module, "LogisticRegressionConfig", reinterpret_cast<PyObject*>(&pyLogisticRegressionConfig::wrapper_Type)) < 0) goto err;
  if (PyModule_AddObject(module, "LogisticRegression", reinterpret_cast<PyObject*>(&pyLogisticRegression::wrapper_Type)) < 0) goto err;
  return module;
err:
  Py_DECREF(module);
  return nullptr;
}

}  // namespace __logistic__regression_clifwrap

namespace kaldi {
using namespace ::clif;
using ::clif::Clif_PyObjAs;
using ::clif::Clif_PyObjFrom;

// LogisticRegression to/from ::kaldi::LogisticRegression conversion

bool Clif_PyObjAs(PyObject* py, ::kaldi::LogisticRegression** c) {
  assert(c != nullptr);
  if (Py_None == py) {
    *c = nullptr;
    return true;
  }
  ::kaldi::LogisticRegression* cpp = __logistic__regression_clifwrap::pyLogisticRegression::ThisPtr(py);
  if (cpp == nullptr) return false;
  *c = cpp;
  return true;
}

bool Clif_PyObjAs(PyObject* py, std::shared_ptr<::kaldi::LogisticRegression>* c) {
  assert(c != nullptr);
  ::kaldi::LogisticRegression* cpp = __logistic__regression_clifwrap::pyLogisticRegression::ThisPtr(py);
  if (cpp == nullptr) return false;
  *c = ::clif::MakeStdShared(reinterpret_cast<__logistic__regression_clifwrap::pyLogisticRegression::wrapper*>(py)->cpp, cpp);
  return true;
}

bool Clif_PyObjAs(PyObject* py, std::unique_ptr<::kaldi::LogisticRegression>* c) {
  assert(c != nullptr);
  ::kaldi::LogisticRegression* cpp = __logistic__regression_clifwrap::pyLogisticRegression::ThisPtr(py);
  if (cpp == nullptr) return false;
  if (!reinterpret_cast<__logistic__regression_clifwrap::pyLogisticRegression::wrapper*>(py)->cpp.Detach()) {
    PyErr_SetString(PyExc_ValueError, "Cannot convert LogisticRegression instance to std::unique_ptr.");
    return false;
  }
  c->reset(cpp);
  return true;
}

bool Clif_PyObjAs(PyObject* py, ::kaldi::LogisticRegression* c) {
  assert(c != nullptr);
  ::kaldi::LogisticRegression* cpp = __logistic__regression_clifwrap::pyLogisticRegression::ThisPtr(py);
  if (cpp == nullptr) return false;
  *c = *cpp;
  return true;
}

bool Clif_PyObjAs(PyObject* py, ::gtl::optional<::kaldi::LogisticRegression>* c) {
  assert(c != nullptr);
  ::kaldi::LogisticRegression* cpp = __logistic__regression_clifwrap::pyLogisticRegression::ThisPtr(py);
  if (cpp == nullptr) return false;
  *c = *cpp;
  return true;
}

PyObject* Clif_PyObjFrom(::kaldi::LogisticRegression* c, py::PostConv unused) {
  if (c == nullptr) Py_RETURN_NONE;
  PyObject* py = PyType_GenericNew(&__logistic__regression_clifwrap::pyLogisticRegression::wrapper_Type, NULL, NULL);
  reinterpret_cast<__logistic__regression_clifwrap::pyLogisticRegression::wrapper*>(py)->cpp = ::clif::Instance<::kaldi::LogisticRegression>(c, ::clif::UnOwnedResource());
  return py;
}

PyObject* Clif_PyObjFrom(std::shared_ptr<::kaldi::LogisticRegression> c, py::PostConv unused) {
  if (c == nullptr) Py_RETURN_NONE;
  PyObject* py = PyType_GenericNew(&__logistic__regression_clifwrap::pyLogisticRegression::wrapper_Type, NULL, NULL);
  reinterpret_cast<__logistic__regression_clifwrap::pyLogisticRegression::wrapper*>(py)->cpp = ::clif::Instance<::kaldi::LogisticRegression>(c);
  return py;
}

PyObject* Clif_PyObjFrom(std::unique_ptr<::kaldi::LogisticRegression> c, py::PostConv unused) {
  if (c == nullptr) Py_RETURN_NONE;
  PyObject* py = PyType_GenericNew(&__logistic__regression_clifwrap::pyLogisticRegression::wrapper_Type, NULL, NULL);
  reinterpret_cast<__logistic__regression_clifwrap::pyLogisticRegression::wrapper*>(py)->cpp = ::clif::Instance<::kaldi::LogisticRegression>(std::move(c));
  return py;
}

PyObject* Clif_PyObjFrom(const ::kaldi::LogisticRegression& c, py::PostConv unused) {
  PyObject* py = PyType_GenericNew(&__logistic__regression_clifwrap::pyLogisticRegression::wrapper_Type, NULL, NULL);
  reinterpret_cast<__logistic__regression_clifwrap::pyLogisticRegression::wrapper*>(py)->cpp = ::clif::MakeShared<::kaldi::LogisticRegression>(c);
  return py;
}

// LogisticRegressionConfig to/from ::kaldi::LogisticRegressionConfig conversion

bool Clif_PyObjAs(PyObject* py, ::kaldi::LogisticRegressionConfig** c) {
  assert(c != nullptr);
  if (Py_None == py) {
    *c = nullptr;
    return true;
  }
  ::kaldi::LogisticRegressionConfig* cpp = __logistic__regression_clifwrap::pyLogisticRegressionConfig::ThisPtr(py);
  if (cpp == nullptr) return false;
  *c = cpp;
  return true;
}

bool Clif_PyObjAs(PyObject* py, std::shared_ptr<::kaldi::LogisticRegressionConfig>* c) {
  assert(c != nullptr);
  ::kaldi::LogisticRegressionConfig* cpp = __logistic__regression_clifwrap::pyLogisticRegressionConfig::ThisPtr(py);
  if (cpp == nullptr) return false;
  *c = ::clif::MakeStdShared(reinterpret_cast<__logistic__regression_clifwrap::pyLogisticRegressionConfig::wrapper*>(py)->cpp, cpp);
  return true;
}

bool Clif_PyObjAs(PyObject* py, std::unique_ptr<::kaldi::LogisticRegressionConfig>* c) {
  assert(c != nullptr);
  ::kaldi::LogisticRegressionConfig* cpp = __logistic__regression_clifwrap::pyLogisticRegressionConfig::ThisPtr(py);
  if (cpp == nullptr) return false;
  if (!reinterpret_cast<__logistic__regression_clifwrap::pyLogisticRegressionConfig::wrapper*>(py)->cpp.Detach()) {
    PyErr_SetString(PyExc_ValueError, "Cannot convert LogisticRegressionConfig instance to std::unique_ptr.");
    return false;
  }
  c->reset(cpp);
  return true;
}

bool Clif_PyObjAs(PyObject* py, ::kaldi::LogisticRegressionConfig* c) {
  assert(c != nullptr);
  ::kaldi::LogisticRegressionConfig* cpp = __logistic__regression_clifwrap::pyLogisticRegressionConfig::ThisPtr(py);
  if (cpp == nullptr) return false;
  *c = *cpp;
  return true;
}

bool Clif_PyObjAs(PyObject* py, ::gtl::optional<::kaldi::LogisticRegressionConfig>* c) {
  assert(c != nullptr);
  ::kaldi::LogisticRegressionConfig* cpp = __logistic__regression_clifwrap::pyLogisticRegressionConfig::ThisPtr(py);
  if (cpp == nullptr) return false;
  *c = *cpp;
  return true;
}

PyObject* Clif_PyObjFrom(::kaldi::LogisticRegressionConfig* c, py::PostConv unused) {
  if (c == nullptr) Py_RETURN_NONE;
  PyObject* py = PyType_GenericNew(&__logistic__regression_clifwrap::pyLogisticRegressionConfig::wrapper_Type, NULL, NULL);
  reinterpret_cast<__logistic__regression_clifwrap::pyLogisticRegressionConfig::wrapper*>(py)->cpp = ::clif::Instance<::kaldi::LogisticRegressionConfig>(c, ::clif::UnOwnedResource());
  return py;
}

PyObject* Clif_PyObjFrom(std::shared_ptr<::kaldi::LogisticRegressionConfig> c, py::PostConv unused) {
  if (c == nullptr) Py_RETURN_NONE;
  PyObject* py = PyType_GenericNew(&__logistic__regression_clifwrap::pyLogisticRegressionConfig::wrapper_Type, NULL, NULL);
  reinterpret_cast<__logistic__regression_clifwrap::pyLogisticRegressionConfig::wrapper*>(py)->cpp = ::clif::Instance<::kaldi::LogisticRegressionConfig>(c);
  return py;
}

PyObject* Clif_PyObjFrom(std::unique_ptr<::kaldi::LogisticRegressionConfig> c, py::PostConv unused) {
  if (c == nullptr) Py_RETURN_NONE;
  PyObject* py = PyType_GenericNew(&__logistic__regression_clifwrap::pyLogisticRegressionConfig::wrapper_Type, NULL, NULL);
  reinterpret_cast<__logistic__regression_clifwrap::pyLogisticRegressionConfig::wrapper*>(py)->cpp = ::clif::Instance<::kaldi::LogisticRegressionConfig>(std::move(c));
  return py;
}

PyObject* Clif_PyObjFrom(const ::kaldi::LogisticRegressionConfig& c, py::PostConv unused) {
  PyObject* py = PyType_GenericNew(&__logistic__regression_clifwrap::pyLogisticRegressionConfig::wrapper_Type, NULL, NULL);
  reinterpret_cast<__logistic__regression_clifwrap::pyLogisticRegressionConfig::wrapper*>(py)->cpp = ::clif::MakeShared<::kaldi::LogisticRegressionConfig>(c);
  return py;
}

}  // namespace kaldi
