//////////////////////////////////////////////////////////////////////
// This file was automatically generated by CLIF to run under Python 3
// Version 0.3
//////////////////////////////////////////////////////////////////////
// source: /pykaldi/kaldi/rnnlm/rnnlm-embedding-training.clif

#include <Python.h>
#include "clif/python/ptr_util.h"
#include "clif/python/optional.h"
#include "clif/python/types.h"
#include "itf/options-itf-clifwrap.h"
#include "cudamatrix/cu-matrixdim-clifwrap.h"
#include "cudamatrix/cu-array-clifwrap.h"
#include "cudamatrix/cu-matrix-clifwrap.h"
#include "rnnlm-embedding-training-clifwrap.h"
#include "clif/python/stltypes.h"
#include "clif/python/slots.h"

namespace __rnnlm__embedding__training_clifwrap {
using namespace clif;

#define _0 py::postconv::PASS
#define _1 UnicodeFromBytes
#define _2 UnicodeFromBytes


namespace pyRnnlmEmbeddingTrainerOptions {

struct wrapper {
  PyObject_HEAD
  ::clif::Instance<::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions> cpp;
};
static ::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions* ThisPtr(PyObject*);

static PyObject* get_print_interval(PyObject* self, void* xdata) {
  auto cpp = ThisPtr(self); if (!cpp) return nullptr;
  return Clif_PyObjFrom(cpp->print_interval, {});
}

static int set_print_interval(PyObject* self, PyObject* value, void* xdata) {
  if (value == nullptr) {
    PyErr_SetString(PyExc_TypeError, "Cannot delete the print_interval attribute");
    return -1;
  }
  auto cpp = ThisPtr(self); if (!cpp) return -1;
  if (Clif_PyObjAs(value, &cpp->print_interval)) return 0;
  PyObject* s = PyObject_Repr(value);
  PyErr_Format(PyExc_ValueError, "%s is not valid for print_interval:int", s? PyUnicode_AsUTF8(s): "input");
  Py_XDECREF(s);
  return -1;
}

static PyObject* get_momentum(PyObject* self, void* xdata) {
  auto cpp = ThisPtr(self); if (!cpp) return nullptr;
  return Clif_PyObjFrom(cpp->momentum, {});
}

static int set_momentum(PyObject* self, PyObject* value, void* xdata) {
  if (value == nullptr) {
    PyErr_SetString(PyExc_TypeError, "Cannot delete the momentum attribute");
    return -1;
  }
  auto cpp = ThisPtr(self); if (!cpp) return -1;
  if (Clif_PyObjAs(value, &cpp->momentum)) return 0;
  PyObject* s = PyObject_Repr(value);
  PyErr_Format(PyExc_ValueError, "%s is not valid for momentum:float", s? PyUnicode_AsUTF8(s): "input");
  Py_XDECREF(s);
  return -1;
}

static PyObject* get_max_param_change(PyObject* self, void* xdata) {
  auto cpp = ThisPtr(self); if (!cpp) return nullptr;
  return Clif_PyObjFrom(cpp->max_param_change, {});
}

static int set_max_param_change(PyObject* self, PyObject* value, void* xdata) {
  if (value == nullptr) {
    PyErr_SetString(PyExc_TypeError, "Cannot delete the max_param_change attribute");
    return -1;
  }
  auto cpp = ThisPtr(self); if (!cpp) return -1;
  if (Clif_PyObjAs(value, &cpp->max_param_change)) return 0;
  PyObject* s = PyObject_Repr(value);
  PyErr_Format(PyExc_ValueError, "%s is not valid for max_param_change:float", s? PyUnicode_AsUTF8(s): "input");
  Py_XDECREF(s);
  return -1;
}

static PyObject* get_l2_regularize(PyObject* self, void* xdata) {
  auto cpp = ThisPtr(self); if (!cpp) return nullptr;
  return Clif_PyObjFrom(cpp->l2_regularize, {});
}

static int set_l2_regularize(PyObject* self, PyObject* value, void* xdata) {
  if (value == nullptr) {
    PyErr_SetString(PyExc_TypeError, "Cannot delete the l2_regularize attribute");
    return -1;
  }
  auto cpp = ThisPtr(self); if (!cpp) return -1;
  if (Clif_PyObjAs(value, &cpp->l2_regularize)) return 0;
  PyObject* s = PyObject_Repr(value);
  PyErr_Format(PyExc_ValueError, "%s is not valid for l2_regularize:float", s? PyUnicode_AsUTF8(s): "input");
  Py_XDECREF(s);
  return -1;
}

static PyObject* get_learning_rate(PyObject* self, void* xdata) {
  auto cpp = ThisPtr(self); if (!cpp) return nullptr;
  return Clif_PyObjFrom(cpp->learning_rate, {});
}

static int set_learning_rate(PyObject* self, PyObject* value, void* xdata) {
  if (value == nullptr) {
    PyErr_SetString(PyExc_TypeError, "Cannot delete the learning_rate attribute");
    return -1;
  }
  auto cpp = ThisPtr(self); if (!cpp) return -1;
  if (Clif_PyObjAs(value, &cpp->learning_rate)) return 0;
  PyObject* s = PyObject_Repr(value);
  PyErr_Format(PyExc_ValueError, "%s is not valid for learning_rate:float", s? PyUnicode_AsUTF8(s): "input");
  Py_XDECREF(s);
  return -1;
}

static PyObject* get_backstitch_training_scale(PyObject* self, void* xdata) {
  auto cpp = ThisPtr(self); if (!cpp) return nullptr;
  return Clif_PyObjFrom(cpp->backstitch_training_scale, {});
}

static int set_backstitch_training_scale(PyObject* self, PyObject* value, void* xdata) {
  if (value == nullptr) {
    PyErr_SetString(PyExc_TypeError, "Cannot delete the backstitch_training_scale attribute");
    return -1;
  }
  auto cpp = ThisPtr(self); if (!cpp) return -1;
  if (Clif_PyObjAs(value, &cpp->backstitch_training_scale)) return 0;
  PyObject* s = PyObject_Repr(value);
  PyErr_Format(PyExc_ValueError, "%s is not valid for backstitch_training_scale:float", s? PyUnicode_AsUTF8(s): "input");
  Py_XDECREF(s);
  return -1;
}

static PyObject* get_backstitch_training_interval(PyObject* self, void* xdata) {
  auto cpp = ThisPtr(self); if (!cpp) return nullptr;
  return Clif_PyObjFrom(cpp->backstitch_training_interval, {});
}

static int set_backstitch_training_interval(PyObject* self, PyObject* value, void* xdata) {
  if (value == nullptr) {
    PyErr_SetString(PyExc_TypeError, "Cannot delete the backstitch_training_interval attribute");
    return -1;
  }
  auto cpp = ThisPtr(self); if (!cpp) return -1;
  if (Clif_PyObjAs(value, &cpp->backstitch_training_interval)) return 0;
  PyObject* s = PyObject_Repr(value);
  PyErr_Format(PyExc_ValueError, "%s is not valid for backstitch_training_interval:int", s? PyUnicode_AsUTF8(s): "input");
  Py_XDECREF(s);
  return -1;
}

static PyObject* get_use_natural_gradient(PyObject* self, void* xdata) {
  auto cpp = ThisPtr(self); if (!cpp) return nullptr;
  return Clif_PyObjFrom(cpp->use_natural_gradient, {});
}

static int set_use_natural_gradient(PyObject* self, PyObject* value, void* xdata) {
  if (value == nullptr) {
    PyErr_SetString(PyExc_TypeError, "Cannot delete the use_natural_gradient attribute");
    return -1;
  }
  auto cpp = ThisPtr(self); if (!cpp) return -1;
  if (Clif_PyObjAs(value, &cpp->use_natural_gradient)) return 0;
  PyObject* s = PyObject_Repr(value);
  PyErr_Format(PyExc_ValueError, "%s is not valid for use_natural_gradient:bool", s? PyUnicode_AsUTF8(s): "input");
  Py_XDECREF(s);
  return -1;
}

static PyObject* get_natural_gradient_alpha(PyObject* self, void* xdata) {
  auto cpp = ThisPtr(self); if (!cpp) return nullptr;
  return Clif_PyObjFrom(cpp->natural_gradient_alpha, {});
}

static int set_natural_gradient_alpha(PyObject* self, PyObject* value, void* xdata) {
  if (value == nullptr) {
    PyErr_SetString(PyExc_TypeError, "Cannot delete the natural_gradient_alpha attribute");
    return -1;
  }
  auto cpp = ThisPtr(self); if (!cpp) return -1;
  if (Clif_PyObjAs(value, &cpp->natural_gradient_alpha)) return 0;
  PyObject* s = PyObject_Repr(value);
  PyErr_Format(PyExc_ValueError, "%s is not valid for natural_gradient_alpha:float", s? PyUnicode_AsUTF8(s): "input");
  Py_XDECREF(s);
  return -1;
}

static PyObject* get_natural_gradient_rank(PyObject* self, void* xdata) {
  auto cpp = ThisPtr(self); if (!cpp) return nullptr;
  return Clif_PyObjFrom(cpp->natural_gradient_rank, {});
}

static int set_natural_gradient_rank(PyObject* self, PyObject* value, void* xdata) {
  if (value == nullptr) {
    PyErr_SetString(PyExc_TypeError, "Cannot delete the natural_gradient_rank attribute");
    return -1;
  }
  auto cpp = ThisPtr(self); if (!cpp) return -1;
  if (Clif_PyObjAs(value, &cpp->natural_gradient_rank)) return 0;
  PyObject* s = PyObject_Repr(value);
  PyErr_Format(PyExc_ValueError, "%s is not valid for natural_gradient_rank:int", s? PyUnicode_AsUTF8(s): "input");
  Py_XDECREF(s);
  return -1;
}

static PyObject* get_natural_gradient_update_period(PyObject* self, void* xdata) {
  auto cpp = ThisPtr(self); if (!cpp) return nullptr;
  return Clif_PyObjFrom(cpp->natural_gradient_update_period, {});
}

static int set_natural_gradient_update_period(PyObject* self, PyObject* value, void* xdata) {
  if (value == nullptr) {
    PyErr_SetString(PyExc_TypeError, "Cannot delete the natural_gradient_update_period attribute");
    return -1;
  }
  auto cpp = ThisPtr(self); if (!cpp) return -1;
  if (Clif_PyObjAs(value, &cpp->natural_gradient_update_period)) return 0;
  PyObject* s = PyObject_Repr(value);
  PyErr_Format(PyExc_ValueError, "%s is not valid for natural_gradient_update_period:int", s? PyUnicode_AsUTF8(s): "input");
  Py_XDECREF(s);
  return -1;
}

static PyObject* get_natural_gradient_num_minibatches_history(PyObject* self, void* xdata) {
  auto cpp = ThisPtr(self); if (!cpp) return nullptr;
  return Clif_PyObjFrom(cpp->natural_gradient_num_minibatches_history, {});
}

static int set_natural_gradient_num_minibatches_history(PyObject* self, PyObject* value, void* xdata) {
  if (value == nullptr) {
    PyErr_SetString(PyExc_TypeError, "Cannot delete the natural_gradient_num_minibatches_history attribute");
    return -1;
  }
  auto cpp = ThisPtr(self); if (!cpp) return -1;
  if (Clif_PyObjAs(value, &cpp->natural_gradient_num_minibatches_history)) return 0;
  PyObject* s = PyObject_Repr(value);
  PyErr_Format(PyExc_ValueError, "%s is not valid for natural_gradient_num_minibatches_history:int", s? PyUnicode_AsUTF8(s): "input");
  Py_XDECREF(s);
  return -1;
}

// register(opts:OptionsItf)
static PyObject* wrapRegister_as_register(PyObject* self, PyObject* args, PyObject* kw) {
  PyObject* a[1];
  char* names[] = {
      C("opts"),
      nullptr
  };
  if (!PyArg_ParseTupleAndKeywords(args, kw, "O:register", names, &a[0])) return nullptr;
  ::kaldi::OptionsItf * arg1;
  if (!Clif_PyObjAs(a[0], &arg1)) return ArgError("register", names[0], "::kaldi::OptionsItf *", a[0]);
  // Call actual C++ method.
  ::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions* c = ThisPtr(self);
  if (!c) return nullptr;
  Py_INCREF(args);
  Py_XINCREF(kw);
  PyThreadState* _save;
  Py_UNBLOCK_THREADS
  PyObject* err_type = nullptr;
  string err_msg{"C++ exception"};
  try {
    c->Register(arg1);
  } catch(const std::exception& e) {
    err_type = PyExc_RuntimeError;
    err_msg += string(": ") + e.what();
  } catch (...) {
    err_type = PyExc_RuntimeError;
  }
  Py_BLOCK_THREADS
  Py_DECREF(args);
  Py_XDECREF(kw);
  if (err_type) {
    PyErr_SetString(err_type, err_msg.c_str());
    return nullptr;
  }
  Py_RETURN_NONE;
}

// check()
static PyObject* wrapCheck_as_check(PyObject* self) {
  // Call actual C++ method.
  ::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions* c = ThisPtr(self);
  if (!c) return nullptr;
  PyThreadState* _save;
  Py_UNBLOCK_THREADS
  PyObject* err_type = nullptr;
  string err_msg{"C++ exception"};
  try {
    c->Check();
  } catch(const std::exception& e) {
    err_type = PyExc_RuntimeError;
    err_msg += string(": ") + e.what();
  } catch (...) {
    err_type = PyExc_RuntimeError;
  }
  Py_BLOCK_THREADS
  if (err_type) {
    PyErr_SetString(err_type, err_msg.c_str());
    return nullptr;
  }
  Py_RETURN_NONE;
}

static PyGetSetDef Properties[] = {
  {C("print_interval"), get_print_interval, set_print_interval, C("The log printing interval (in terms of #minibatches).")},
  {C("momentum"), get_momentum, set_momentum, C("Momentum constant for training of embeddings (e.g. 0.5 or 0.9).\n\nWe automatically multiply the learning rate by (1-momentum) so that the\n'effective' learning rate is the same as  before (because momentum would\nnormally increase the effective learning rate by 1/(1-momentum)).")},
  {C("max_param_change"), get_max_param_change, set_max_param_change, C("The maximum change in embedding parameters allowed per minibatch.\n\nThis is measured in Euclidean norm. The embedding matrix has dimensions\nnum-features by embedding-dim or num-words by embedding-dim if we're not\nusing a feature-based representation.")},
  {C("l2_regularize"), get_l2_regularize, set_l2_regularize, C("Factor that affects the strength of l2 regularization.\n\nThis affects the strength of l2 regularization on embedding parameters.")},
  {C("learning_rate"), get_learning_rate, set_learning_rate, C("The learning rate used in training the word-embedding matrix.")},
  {C("backstitch_training_scale"), get_backstitch_training_scale, set_backstitch_training_scale, C("Backstitch training factor (alpha).\n\nIf 0 then in the normal training mode.")},
  {C("backstitch_training_interval"), get_backstitch_training_interval, set_backstitch_training_interval, C("Backstitch training interval (n).\n\nDo backstitch training with the specified interval of minibatches.")},
  {C("use_natural_gradient"), get_use_natural_gradient, set_use_natural_gradient, C("Whether to use natural gradient to update the embedding matrix.")},
  {C("natural_gradient_alpha"), get_natural_gradient_alpha, set_natural_gradient_alpha, C("Smoothing constant alpha to use for natural gradient.")},
  {C("natural_gradient_rank"), get_natural_gradient_rank, set_natural_gradient_rank, C("Rank of the Fisher matrix in natural gradient.\n\nThis is applied to learning the embedding matrix (this is in the\nembedding space, so the rank should probably be less than the embedding\ndimension.")},
  {C("natural_gradient_update_period"), get_natural_gradient_update_period, set_natural_gradient_update_period, C("Determines how often the Fisher matrix is updated for natural gradient\nas applied to the embedding matrix.")},
  {C("natural_gradient_num_minibatches_history"), get_natural_gradient_num_minibatches_history, set_natural_gradient_num_minibatches_history, C("Determines how quickly the Fisher estimate for the natural gradient\nis updated, when training the word embedding.")},
  {}
};

static PyMethodDef Methods[] = {
  {C("register"), (PyCFunction)wrapRegister_as_register, METH_VARARGS | METH_KEYWORDS, C("register(opts:OptionsItf)\n\nRegisters options with an object implementing the options interface.\n\nArgs:\n  opts (OptionsItf): An object implementing the options interface.\n    Typically a command-line option parser.")},
  {C("check"), (PyCFunction)wrapCheck_as_check, METH_NOARGS, C("check()\n\nValidates RNNLM embedding training options.")},
  {}
};

// RnnlmEmbeddingTrainerOptions __init__
static int _ctor(PyObject* self, PyObject* args, PyObject* kw);

// RnnlmEmbeddingTrainerOptions __new__
static PyObject* _new(PyTypeObject* type, Py_ssize_t nitems);

// RnnlmEmbeddingTrainerOptions __del__
static void _del(void* self) {
  delete reinterpret_cast<wrapper*>(self);
}

PyTypeObject wrapper_Type = {
  PyVarObject_HEAD_INIT(&PyType_Type, 0)
  "_rnnlm_embedding_training.RnnlmEmbeddingTrainerOptions", // tp_name
  sizeof(wrapper),                     // tp_basicsize
  0,                                   // tp_itemsize
  nullptr,                             // tp_dealloc
  nullptr,                             // tp_print
  nullptr,                             // tp_getattr
  nullptr,                             // tp_setattr
  nullptr,                             // tp_compare
  nullptr,                             // tp_repr
  nullptr,                             // tp_as_number
  nullptr,                             // tp_as_sequence
  nullptr,                             // tp_as_mapping
  nullptr,                             // tp_hash
  nullptr,                             // tp_call
  nullptr,                             // tp_str
  nullptr,                             // tp_getattro
  nullptr,                             // tp_setattro
  nullptr,                             // tp_as_buffer
  Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE, // tp_flags
  "Options for RNNLM embedding training.", // tp_doc
  nullptr,                             // tp_traverse
  nullptr,                             // tp_clear
  nullptr,                             // tp_richcompare
  0,                                   // tp_weaklistoffset
  nullptr,                             // tp_iter
  nullptr,                             // tp_iternext
  Methods,                             // tp_methods
  nullptr,                             // tp_members
  Properties,                          // tp_getset
  nullptr,                             // tp_base
  nullptr,                             // tp_dict
  nullptr,                             // tp_descr_get
  nullptr,                             // tp_descr_set
  0,                                   // tp_dictoffset
  _ctor,                               // tp_init
  _new,                                // tp_alloc
  PyType_GenericNew,                   // tp_new
  _del,                                // tp_free
  nullptr,                             // tp_is_gc
  nullptr,                             // tp_bases
  nullptr,                             // tp_mro
  nullptr,                             // tp_cache
  nullptr,                             // tp_subclasses
  nullptr,                             // tp_weaklist
  nullptr,                             // tp_del
  0,                                   // tp_version_tag
};

static int _ctor(PyObject* self, PyObject* args, PyObject* kw) {
  if ((args && PyTuple_GET_SIZE(args) != 0) || (kw && PyDict_Size(kw) != 0)) {
    PyErr_SetString(PyExc_TypeError, "RnnlmEmbeddingTrainerOptions takes no arguments");
    return -1;
  }
  reinterpret_cast<wrapper*>(self)->cpp = ::clif::MakeShared<::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions>();
  return 0;
}

static PyObject* _new(PyTypeObject* type, Py_ssize_t nitems) {
  assert(nitems == 0);
  PyObject* self = reinterpret_cast<PyObject*>(new wrapper);
  return PyObject_Init(self, &wrapper_Type);
}

static ::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions* ThisPtr(PyObject* py) {
  if (Py_TYPE(py) == &wrapper_Type) {
    return ::clif::python::Get(reinterpret_cast<wrapper*>(py)->cpp);
  }
  PyObject* base = PyObject_CallMethod(py, C("as_kaldi_rnnlm_RnnlmEmbeddingTrainerOptions"), nullptr);
  if (base) {
    if (PyCapsule_CheckExact(base)) {
      void* p = PyCapsule_GetPointer(base, C("::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions"));
      if (!PyErr_Occurred()) {
        ::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions* c = static_cast<::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions*>(p);
        Py_DECREF(base);
        return c;
      }
    }
    Py_DECREF(base);
  }
  if (PyObject_IsInstance(py, reinterpret_cast<PyObject*>(&wrapper_Type))) {
    if (!base) {
      PyErr_Clear();
      return ::clif::python::Get(reinterpret_cast<wrapper*>(py)->cpp);
    }
    PyErr_Format(PyExc_ValueError, "can't convert %s %s to ::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions*", ClassName(py), ClassType(py));
  } else {
    PyErr_Format(PyExc_TypeError, "expecting %s instance, got %s %s", wrapper_Type.tp_name, ClassName(py), ClassType(py));
  }
  return nullptr;
}
}  // namespace pyRnnlmEmbeddingTrainerOptions

namespace pyRnnlmEmbeddingTrainer {

struct wrapper {
  PyObject_HEAD
  ::clif::Instance<::kaldi::rnnlm::RnnlmEmbeddingTrainer> cpp;
};
static ::kaldi::rnnlm::RnnlmEmbeddingTrainer* ThisPtr(PyObject*);

// __init__(config:RnnlmEmbeddingTrainerOptions, embedding_mat:CuMatrix)
static PyObject* wrapRnnlmEmbeddingTrainer_as___init__(PyObject* self, PyObject* args, PyObject* kw) {
  PyObject* a[2];
  char* names[] = {
      C("config"),
      C("embedding_mat"),
      nullptr
  };
  if (!PyArg_ParseTupleAndKeywords(args, kw, "OO:__init__", names, &a[0], &a[1])) return nullptr;
  ::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions* arg1;
  if (!Clif_PyObjAs(a[0], &arg1)) return ArgError("__init__", names[0], "::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions", a[0]);
  ::kaldi::CuMatrix<float> * arg2;
  if (!Clif_PyObjAs(a[1], &arg2)) return ArgError("__init__", names[1], "::kaldi::CuMatrix<float> *", a[1]);
  // Call actual C++ method.
  Py_INCREF(args);
  Py_XINCREF(kw);
  PyThreadState* _save;
  Py_UNBLOCK_THREADS
  PyObject* err_type = nullptr;
  string err_msg{"C++ exception"};
  try {
    reinterpret_cast<wrapper*>(self)->cpp = ::clif::MakeShared<::kaldi::rnnlm::RnnlmEmbeddingTrainer>(*arg1, arg2);
  } catch(const std::exception& e) {
    err_type = PyExc_RuntimeError;
    err_msg += string(": ") + e.what();
  } catch (...) {
    err_type = PyExc_RuntimeError;
  }
  Py_BLOCK_THREADS
  Py_DECREF(args);
  Py_XDECREF(kw);
  if (err_type) {
    PyErr_SetString(err_type, err_msg.c_str());
    return nullptr;
  }
  Py_RETURN_NONE;
}

// train(embedding_deriv:CuMatrixBase)
static PyObject* wrapTrain_as_train(PyObject* self, PyObject* args, PyObject* kw) {
  PyObject* a[1];
  char* names[] = {
      C("embedding_deriv"),
      nullptr
  };
  if (!PyArg_ParseTupleAndKeywords(args, kw, "O:train", names, &a[0])) return nullptr;
  ::kaldi::CuMatrixBase<float> * arg1;
  if (!Clif_PyObjAs(a[0], &arg1)) return ArgError("train", names[0], "::kaldi::CuMatrixBase<float> *", a[0]);
  // Call actual C++ method.
  ::kaldi::rnnlm::RnnlmEmbeddingTrainer* c = ThisPtr(self);
  if (!c) return nullptr;
  Py_INCREF(args);
  Py_XINCREF(kw);
  PyThreadState* _save;
  Py_UNBLOCK_THREADS
  PyObject* err_type = nullptr;
  string err_msg{"C++ exception"};
  try {
    c->Train(arg1);
  } catch(const std::exception& e) {
    err_type = PyExc_RuntimeError;
    err_msg += string(": ") + e.what();
  } catch (...) {
    err_type = PyExc_RuntimeError;
  }
  Py_BLOCK_THREADS
  Py_DECREF(args);
  Py_XDECREF(kw);
  if (err_type) {
    PyErr_SetString(err_type, err_msg.c_str());
    return nullptr;
  }
  Py_RETURN_NONE;
}

// train_backstitch(is_backstitch_step1:bool, embedding_deriv:CuMatrixBase)
static PyObject* wrapTrainBackstitch_as_train_backstitch(PyObject* self, PyObject* args, PyObject* kw) {
  PyObject* a[2];
  char* names[] = {
      C("is_backstitch_step1"),
      C("embedding_deriv"),
      nullptr
  };
  if (!PyArg_ParseTupleAndKeywords(args, kw, "OO:train_backstitch", names, &a[0], &a[1])) return nullptr;
  bool arg1;
  if (!Clif_PyObjAs(a[0], &arg1)) return ArgError("train_backstitch", names[0], "bool", a[0]);
  ::kaldi::CuMatrixBase<float> * arg2;
  if (!Clif_PyObjAs(a[1], &arg2)) return ArgError("train_backstitch", names[1], "::kaldi::CuMatrixBase<float> *", a[1]);
  // Call actual C++ method.
  ::kaldi::rnnlm::RnnlmEmbeddingTrainer* c = ThisPtr(self);
  if (!c) return nullptr;
  Py_INCREF(args);
  Py_XINCREF(kw);
  PyThreadState* _save;
  Py_UNBLOCK_THREADS
  PyObject* err_type = nullptr;
  string err_msg{"C++ exception"};
  try {
    c->TrainBackstitch(std::move(arg1), arg2);
  } catch(const std::exception& e) {
    err_type = PyExc_RuntimeError;
    err_msg += string(": ") + e.what();
  } catch (...) {
    err_type = PyExc_RuntimeError;
  }
  Py_BLOCK_THREADS
  Py_DECREF(args);
  Py_XDECREF(kw);
  if (err_type) {
    PyErr_SetString(err_type, err_msg.c_str());
    return nullptr;
  }
  Py_RETURN_NONE;
}

// train_with_subsampling(active_words:CuArray, word_embedding_deriv:CuMatrixBase)
static PyObject* wrapTrain_as_train_with_subsampling(PyObject* self, PyObject* args, PyObject* kw) {
  PyObject* a[2];
  char* names[] = {
      C("active_words"),
      C("word_embedding_deriv"),
      nullptr
  };
  if (!PyArg_ParseTupleAndKeywords(args, kw, "OO:train_with_subsampling", names, &a[0], &a[1])) return nullptr;
  ::kaldi::CuArray<int>* arg1;
  if (!Clif_PyObjAs(a[0], &arg1)) return ArgError("train_with_subsampling", names[0], "::kaldi::CuArray<int>", a[0]);
  ::kaldi::CuMatrixBase<float> * arg2;
  if (!Clif_PyObjAs(a[1], &arg2)) return ArgError("train_with_subsampling", names[1], "::kaldi::CuMatrixBase<float> *", a[1]);
  // Call actual C++ method.
  ::kaldi::rnnlm::RnnlmEmbeddingTrainer* c = ThisPtr(self);
  if (!c) return nullptr;
  Py_INCREF(args);
  Py_XINCREF(kw);
  PyThreadState* _save;
  Py_UNBLOCK_THREADS
  PyObject* err_type = nullptr;
  string err_msg{"C++ exception"};
  try {
    c->Train(*arg1, arg2);
  } catch(const std::exception& e) {
    err_type = PyExc_RuntimeError;
    err_msg += string(": ") + e.what();
  } catch (...) {
    err_type = PyExc_RuntimeError;
  }
  Py_BLOCK_THREADS
  Py_DECREF(args);
  Py_XDECREF(kw);
  if (err_type) {
    PyErr_SetString(err_type, err_msg.c_str());
    return nullptr;
  }
  Py_RETURN_NONE;
}

// train_backstitch_with_subsampling(is_backstitch_step1:bool, active_words:CuArray, word_embedding_deriv:CuMatrixBase)
static PyObject* wrapTrainBackstitch_as_train_backstitch_with_subsampling(PyObject* self, PyObject* args, PyObject* kw) {
  PyObject* a[3];
  char* names[] = {
      C("is_backstitch_step1"),
      C("active_words"),
      C("word_embedding_deriv"),
      nullptr
  };
  if (!PyArg_ParseTupleAndKeywords(args, kw, "OOO:train_backstitch_with_subsampling", names, &a[0], &a[1], &a[2])) return nullptr;
  bool arg1;
  if (!Clif_PyObjAs(a[0], &arg1)) return ArgError("train_backstitch_with_subsampling", names[0], "bool", a[0]);
  ::kaldi::CuArray<int>* arg2;
  if (!Clif_PyObjAs(a[1], &arg2)) return ArgError("train_backstitch_with_subsampling", names[1], "::kaldi::CuArray<int>", a[1]);
  ::kaldi::CuMatrixBase<float> * arg3;
  if (!Clif_PyObjAs(a[2], &arg3)) return ArgError("train_backstitch_with_subsampling", names[2], "::kaldi::CuMatrixBase<float> *", a[2]);
  // Call actual C++ method.
  ::kaldi::rnnlm::RnnlmEmbeddingTrainer* c = ThisPtr(self);
  if (!c) return nullptr;
  Py_INCREF(args);
  Py_XINCREF(kw);
  PyThreadState* _save;
  Py_UNBLOCK_THREADS
  PyObject* err_type = nullptr;
  string err_msg{"C++ exception"};
  try {
    c->TrainBackstitch(std::move(arg1), *arg2, arg3);
  } catch(const std::exception& e) {
    err_type = PyExc_RuntimeError;
    err_msg += string(": ") + e.what();
  } catch (...) {
    err_type = PyExc_RuntimeError;
  }
  Py_BLOCK_THREADS
  Py_DECREF(args);
  Py_XDECREF(kw);
  if (err_type) {
    PyErr_SetString(err_type, err_msg.c_str());
    return nullptr;
  }
  Py_RETURN_NONE;
}

static PyMethodDef Methods[] = {
  {C("__init__"), (PyCFunction)wrapRnnlmEmbeddingTrainer_as___init__, METH_VARARGS | METH_KEYWORDS, C("__init__(config:RnnlmEmbeddingTrainerOptions, embedding_mat:CuMatrix)\n  Calls C++ function\n  void ::kaldi::rnnlm::RnnlmEmbeddingTrainer::RnnlmEmbeddingTrainer(::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions, ::kaldi::CuMatrix<float> *)")},
  {C("train"), (PyCFunction)wrapTrain_as_train, METH_VARARGS | METH_KEYWORDS, C("train(embedding_deriv:CuMatrixBase)\n\nDo training for one minibatch.\n\nThis version is used either when there is no subsampling, or when there\nis subsampling but we are using a feature representation so the\nsubsampling is handled outside of this code.\n\nArgs:\n  embedding_deriv (CuMatrixBase): The derivative of the objective\n    function w.r.t. the word (or feature) embedding matrix.")},
  {C("train_backstitch"), (PyCFunction)wrapTrainBackstitch_as_train_backstitch, METH_VARARGS | METH_KEYWORDS, C("train_backstitch(is_backstitch_step1:bool, embedding_deriv:CuMatrixBase)\n\nDo backstitch training for one minibatch.\n\nThis version is used either when there is no subsampling, or when there\nis subsampling but we are using a feature representation so the\nsubsampling is handled outside of this code.\n\nDepending on whether is_backstitch_step1 is true, It could be either\nthe first (backward) step, or the second (forward) step of backstitch.\n\nArgs:\n  is_backstitch_step1 (bool): If true update stats otherwise not.\n  embedding_deriv (CuMatrixBase): The derivative of the objective\n    function w.r.t. the word (or feature) embedding matrix.")},
  {C("train_with_subsampling"), (PyCFunction)wrapTrain_as_train_with_subsampling, METH_VARARGS | METH_KEYWORDS, C("train_with_subsampling(active_words:CuArray, word_embedding_deriv:CuMatrixBase)\n\nDo training for one minibatch.\n\nThis version is for when there is subsampling, and the user is\nproviding the derivative w.r.t. just the word-indexes that were used in\nthis minibatch. `active_words` is a sorted, unique list of the\nword-indexes that were used in this minibatch, and\n`word_embedding_deriv` is the derivative w.r.t. the embedding of that\nlist of words.\n\nArgs:\n  active_words (CuArray): A sorted, unique list of the word indexes\n    used, with dimension equal to `word_embedding_deriv.num_rows`.\n  word_embedding_deriv (CuMatrixBase): The derivative of the objective\n    function w.r.t. the word embedding matrix.")},
  {C("train_backstitch_with_subsampling"), (PyCFunction)wrapTrainBackstitch_as_train_backstitch_with_subsampling, METH_VARARGS | METH_KEYWORDS, C("train_backstitch_with_subsampling(is_backstitch_step1:bool, active_words:CuArray, word_embedding_deriv:CuMatrixBase)\n\nDo backstitch training for one minibatch.\n\nThis version is for when there is subsampling, and the user is\nproviding the derivative w.r.t. just the word-indexes that were used in\nthis minibatch. `active_words` is a sorted, unique list of the\nword-indexes that were used in this minibatch, and\n`word_embedding_deriv` is the derivative w.r.t. the embedding of that\nlist of words.\n\nDepending on whether is_backstitch_step1 is true, It could be either\nthe first (backward) step, or the second (forward) step of backstitch.\n\nArgs:\n  is_backstitch_step1 (bool): If true update stats otherwise not.\n  active_words (CuArray): A sorted, unique list of the word indexes\n    used, with dimension equal to `word_embedding_deriv.num_rows`.\n  word_embedding_deriv (CuMatrixBase): The derivative of the objective\n    function w.r.t. the word embedding matrix.")},
  {}
};

// RnnlmEmbeddingTrainer __init__
static int _ctor(PyObject* self, PyObject* args, PyObject* kw);

// RnnlmEmbeddingTrainer __new__
static PyObject* _new(PyTypeObject* type, Py_ssize_t nitems);

// RnnlmEmbeddingTrainer __del__
static void _dtor(PyObject* self) {
  Py_BEGIN_ALLOW_THREADS
  reinterpret_cast<wrapper*>(self)->cpp.Destruct();
  Py_END_ALLOW_THREADS
  Py_TYPE(self)->tp_free(self);
}
static void _del(void* self) {
  delete reinterpret_cast<wrapper*>(self);
}

PyTypeObject wrapper_Type = {
  PyVarObject_HEAD_INIT(&PyType_Type, 0)
  "_rnnlm_embedding_training.RnnlmEmbeddingTrainer", // tp_name
  sizeof(wrapper),                     // tp_basicsize
  0,                                   // tp_itemsize
  _dtor,                               // tp_dealloc
  nullptr,                             // tp_print
  nullptr,                             // tp_getattr
  nullptr,                             // tp_setattr
  nullptr,                             // tp_compare
  nullptr,                             // tp_repr
  nullptr,                             // tp_as_number
  nullptr,                             // tp_as_sequence
  nullptr,                             // tp_as_mapping
  nullptr,                             // tp_hash
  nullptr,                             // tp_call
  nullptr,                             // tp_str
  nullptr,                             // tp_getattro
  nullptr,                             // tp_setattro
  nullptr,                             // tp_as_buffer
  Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE, // tp_flags
  "RNNLM embedding trainer.\n\nThis class is responsible for training the word embedding matrix or\nfeature embedding matrix.\n\nArgs:\n  config (RnnlmEmbeddingTrainerOptions): Options for RNNLM embedding\n    training.\n  embedding_mat (CuMatrix): The embedding matrix to be trained,\n    of dimension (num-words or num-features) by embedding-dim (depending\n    whether we are using a feature representation of words, or not).", // tp_doc
  nullptr,                             // tp_traverse
  nullptr,                             // tp_clear
  nullptr,                             // tp_richcompare
  0,                                   // tp_weaklistoffset
  nullptr,                             // tp_iter
  nullptr,                             // tp_iternext
  Methods,                             // tp_methods
  nullptr,                             // tp_members
  nullptr,                             // tp_getset
  nullptr,                             // tp_base
  nullptr,                             // tp_dict
  nullptr,                             // tp_descr_get
  nullptr,                             // tp_descr_set
  0,                                   // tp_dictoffset
  _ctor,                               // tp_init
  _new,                                // tp_alloc
  PyType_GenericNew,                   // tp_new
  _del,                                // tp_free
  nullptr,                             // tp_is_gc
  nullptr,                             // tp_bases
  nullptr,                             // tp_mro
  nullptr,                             // tp_cache
  nullptr,                             // tp_subclasses
  nullptr,                             // tp_weaklist
  nullptr,                             // tp_del
  0,                                   // tp_version_tag
};

static int _ctor(PyObject* self, PyObject* args, PyObject* kw) {
  PyObject* init = wrapRnnlmEmbeddingTrainer_as___init__(self, args, kw);
  Py_XDECREF(init);
  return init? 0: -1;
}

static PyObject* _new(PyTypeObject* type, Py_ssize_t nitems) {
  assert(nitems == 0);
  PyObject* self = reinterpret_cast<PyObject*>(new wrapper);
  return PyObject_Init(self, &wrapper_Type);
}

static ::kaldi::rnnlm::RnnlmEmbeddingTrainer* ThisPtr(PyObject* py) {
  if (Py_TYPE(py) == &wrapper_Type) {
    return ::clif::python::Get(reinterpret_cast<wrapper*>(py)->cpp);
  }
  PyObject* base = PyObject_CallMethod(py, C("as_kaldi_rnnlm_RnnlmEmbeddingTrainer"), nullptr);
  if (base) {
    if (PyCapsule_CheckExact(base)) {
      void* p = PyCapsule_GetPointer(base, C("::kaldi::rnnlm::RnnlmEmbeddingTrainer"));
      if (!PyErr_Occurred()) {
        ::kaldi::rnnlm::RnnlmEmbeddingTrainer* c = static_cast<::kaldi::rnnlm::RnnlmEmbeddingTrainer*>(p);
        Py_DECREF(base);
        return c;
      }
    }
    Py_DECREF(base);
  }
  if (PyObject_IsInstance(py, reinterpret_cast<PyObject*>(&wrapper_Type))) {
    if (!base) {
      PyErr_Clear();
      return ::clif::python::Get(reinterpret_cast<wrapper*>(py)->cpp);
    }
    PyErr_Format(PyExc_ValueError, "can't convert %s %s to ::kaldi::rnnlm::RnnlmEmbeddingTrainer*", ClassName(py), ClassType(py));
  } else {
    PyErr_Format(PyExc_TypeError, "expecting %s instance, got %s %s", wrapper_Type.tp_name, ClassName(py), ClassType(py));
  }
  return nullptr;
}
}  // namespace pyRnnlmEmbeddingTrainer


// Initialize module

bool Ready() {
  if (PyType_Ready(&pyRnnlmEmbeddingTrainerOptions::wrapper_Type) < 0) return false;
  Py_INCREF(&pyRnnlmEmbeddingTrainerOptions::wrapper_Type);  // For PyModule_AddObject to steal.
  if (PyType_Ready(&pyRnnlmEmbeddingTrainer::wrapper_Type) < 0) return false;
  Py_INCREF(&pyRnnlmEmbeddingTrainer::wrapper_Type);  // For PyModule_AddObject to steal.
  return true;
}

static struct PyModuleDef Module = {
  PyModuleDef_HEAD_INIT,
  "_rnnlm_embedding_training",  // module name
  "CLIF-generated module for rnnlm/rnnlm-embedding-training.h", // module doc
  -1,  // module keeps state in global variables
  nullptr
};

PyObject* Init() {
  PyObject* module = PyModule_Create(&Module);
  if (!module) return nullptr;
  if (PyObject* m = PyImport_ImportModule("_options_itf")) Py_DECREF(m);
  else goto err;
  if (PyObject* m = PyImport_ImportModule("_cu_matrixdim")) Py_DECREF(m);
  else goto err;
  if (PyObject* m = PyImport_ImportModule("_cu_array")) Py_DECREF(m);
  else goto err;
  if (PyObject* m = PyImport_ImportModule("_cu_matrix")) Py_DECREF(m);
  else goto err;
  PyEval_InitThreads();
  if (PyModule_AddObject(module, "RnnlmEmbeddingTrainerOptions", reinterpret_cast<PyObject*>(&pyRnnlmEmbeddingTrainerOptions::wrapper_Type)) < 0) goto err;
  if (PyModule_AddObject(module, "RnnlmEmbeddingTrainer", reinterpret_cast<PyObject*>(&pyRnnlmEmbeddingTrainer::wrapper_Type)) < 0) goto err;
  return module;
err:
  Py_DECREF(module);
  return nullptr;
}

}  // namespace __rnnlm__embedding__training_clifwrap

namespace kaldi { namespace rnnlm {
using namespace ::clif;
using ::clif::Clif_PyObjAs;
using ::clif::Clif_PyObjFrom;

// RnnlmEmbeddingTrainer to/from ::kaldi::rnnlm::RnnlmEmbeddingTrainer conversion

bool Clif_PyObjAs(PyObject* py, ::kaldi::rnnlm::RnnlmEmbeddingTrainer** c) {
  assert(c != nullptr);
  if (Py_None == py) {
    *c = nullptr;
    return true;
  }
  ::kaldi::rnnlm::RnnlmEmbeddingTrainer* cpp = __rnnlm__embedding__training_clifwrap::pyRnnlmEmbeddingTrainer::ThisPtr(py);
  if (cpp == nullptr) return false;
  *c = cpp;
  return true;
}

bool Clif_PyObjAs(PyObject* py, std::shared_ptr<::kaldi::rnnlm::RnnlmEmbeddingTrainer>* c) {
  assert(c != nullptr);
  ::kaldi::rnnlm::RnnlmEmbeddingTrainer* cpp = __rnnlm__embedding__training_clifwrap::pyRnnlmEmbeddingTrainer::ThisPtr(py);
  if (cpp == nullptr) return false;
  *c = ::clif::MakeStdShared(reinterpret_cast<__rnnlm__embedding__training_clifwrap::pyRnnlmEmbeddingTrainer::wrapper*>(py)->cpp, cpp);
  return true;
}

bool Clif_PyObjAs(PyObject* py, std::unique_ptr<::kaldi::rnnlm::RnnlmEmbeddingTrainer>* c) {
  assert(c != nullptr);
  ::kaldi::rnnlm::RnnlmEmbeddingTrainer* cpp = __rnnlm__embedding__training_clifwrap::pyRnnlmEmbeddingTrainer::ThisPtr(py);
  if (cpp == nullptr) return false;
  if (!reinterpret_cast<__rnnlm__embedding__training_clifwrap::pyRnnlmEmbeddingTrainer::wrapper*>(py)->cpp.Detach()) {
    PyErr_SetString(PyExc_ValueError, "Cannot convert RnnlmEmbeddingTrainer instance to std::unique_ptr.");
    return false;
  }
  c->reset(cpp);
  return true;
}

PyObject* Clif_PyObjFrom(::kaldi::rnnlm::RnnlmEmbeddingTrainer* c, py::PostConv unused) {
  if (c == nullptr) Py_RETURN_NONE;
  PyObject* py = PyType_GenericNew(&__rnnlm__embedding__training_clifwrap::pyRnnlmEmbeddingTrainer::wrapper_Type, NULL, NULL);
  reinterpret_cast<__rnnlm__embedding__training_clifwrap::pyRnnlmEmbeddingTrainer::wrapper*>(py)->cpp = ::clif::Instance<::kaldi::rnnlm::RnnlmEmbeddingTrainer>(c, ::clif::UnOwnedResource());
  return py;
}

PyObject* Clif_PyObjFrom(std::shared_ptr<::kaldi::rnnlm::RnnlmEmbeddingTrainer> c, py::PostConv unused) {
  if (c == nullptr) Py_RETURN_NONE;
  PyObject* py = PyType_GenericNew(&__rnnlm__embedding__training_clifwrap::pyRnnlmEmbeddingTrainer::wrapper_Type, NULL, NULL);
  reinterpret_cast<__rnnlm__embedding__training_clifwrap::pyRnnlmEmbeddingTrainer::wrapper*>(py)->cpp = ::clif::Instance<::kaldi::rnnlm::RnnlmEmbeddingTrainer>(c);
  return py;
}

PyObject* Clif_PyObjFrom(std::unique_ptr<::kaldi::rnnlm::RnnlmEmbeddingTrainer> c, py::PostConv unused) {
  if (c == nullptr) Py_RETURN_NONE;
  PyObject* py = PyType_GenericNew(&__rnnlm__embedding__training_clifwrap::pyRnnlmEmbeddingTrainer::wrapper_Type, NULL, NULL);
  reinterpret_cast<__rnnlm__embedding__training_clifwrap::pyRnnlmEmbeddingTrainer::wrapper*>(py)->cpp = ::clif::Instance<::kaldi::rnnlm::RnnlmEmbeddingTrainer>(std::move(c));
  return py;
}

// RnnlmEmbeddingTrainerOptions to/from ::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions conversion

bool Clif_PyObjAs(PyObject* py, ::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions** c) {
  assert(c != nullptr);
  if (Py_None == py) {
    *c = nullptr;
    return true;
  }
  ::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions* cpp = __rnnlm__embedding__training_clifwrap::pyRnnlmEmbeddingTrainerOptions::ThisPtr(py);
  if (cpp == nullptr) return false;
  *c = cpp;
  return true;
}

bool Clif_PyObjAs(PyObject* py, std::shared_ptr<::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions>* c) {
  assert(c != nullptr);
  ::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions* cpp = __rnnlm__embedding__training_clifwrap::pyRnnlmEmbeddingTrainerOptions::ThisPtr(py);
  if (cpp == nullptr) return false;
  *c = ::clif::MakeStdShared(reinterpret_cast<__rnnlm__embedding__training_clifwrap::pyRnnlmEmbeddingTrainerOptions::wrapper*>(py)->cpp, cpp);
  return true;
}

bool Clif_PyObjAs(PyObject* py, std::unique_ptr<::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions>* c) {
  assert(c != nullptr);
  ::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions* cpp = __rnnlm__embedding__training_clifwrap::pyRnnlmEmbeddingTrainerOptions::ThisPtr(py);
  if (cpp == nullptr) return false;
  if (!reinterpret_cast<__rnnlm__embedding__training_clifwrap::pyRnnlmEmbeddingTrainerOptions::wrapper*>(py)->cpp.Detach()) {
    PyErr_SetString(PyExc_ValueError, "Cannot convert RnnlmEmbeddingTrainerOptions instance to std::unique_ptr.");
    return false;
  }
  c->reset(cpp);
  return true;
}

bool Clif_PyObjAs(PyObject* py, ::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions* c) {
  assert(c != nullptr);
  ::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions* cpp = __rnnlm__embedding__training_clifwrap::pyRnnlmEmbeddingTrainerOptions::ThisPtr(py);
  if (cpp == nullptr) return false;
  *c = *cpp;
  return true;
}

bool Clif_PyObjAs(PyObject* py, ::gtl::optional<::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions>* c) {
  assert(c != nullptr);
  ::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions* cpp = __rnnlm__embedding__training_clifwrap::pyRnnlmEmbeddingTrainerOptions::ThisPtr(py);
  if (cpp == nullptr) return false;
  *c = *cpp;
  return true;
}

PyObject* Clif_PyObjFrom(::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions* c, py::PostConv unused) {
  if (c == nullptr) Py_RETURN_NONE;
  PyObject* py = PyType_GenericNew(&__rnnlm__embedding__training_clifwrap::pyRnnlmEmbeddingTrainerOptions::wrapper_Type, NULL, NULL);
  reinterpret_cast<__rnnlm__embedding__training_clifwrap::pyRnnlmEmbeddingTrainerOptions::wrapper*>(py)->cpp = ::clif::Instance<::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions>(c, ::clif::UnOwnedResource());
  return py;
}

PyObject* Clif_PyObjFrom(std::shared_ptr<::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions> c, py::PostConv unused) {
  if (c == nullptr) Py_RETURN_NONE;
  PyObject* py = PyType_GenericNew(&__rnnlm__embedding__training_clifwrap::pyRnnlmEmbeddingTrainerOptions::wrapper_Type, NULL, NULL);
  reinterpret_cast<__rnnlm__embedding__training_clifwrap::pyRnnlmEmbeddingTrainerOptions::wrapper*>(py)->cpp = ::clif::Instance<::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions>(c);
  return py;
}

PyObject* Clif_PyObjFrom(std::unique_ptr<::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions> c, py::PostConv unused) {
  if (c == nullptr) Py_RETURN_NONE;
  PyObject* py = PyType_GenericNew(&__rnnlm__embedding__training_clifwrap::pyRnnlmEmbeddingTrainerOptions::wrapper_Type, NULL, NULL);
  reinterpret_cast<__rnnlm__embedding__training_clifwrap::pyRnnlmEmbeddingTrainerOptions::wrapper*>(py)->cpp = ::clif::Instance<::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions>(std::move(c));
  return py;
}

PyObject* Clif_PyObjFrom(const ::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions& c, py::PostConv unused) {
  PyObject* py = PyType_GenericNew(&__rnnlm__embedding__training_clifwrap::pyRnnlmEmbeddingTrainerOptions::wrapper_Type, NULL, NULL);
  reinterpret_cast<__rnnlm__embedding__training_clifwrap::pyRnnlmEmbeddingTrainerOptions::wrapper*>(py)->cpp = ::clif::MakeShared<::kaldi::rnnlm::RnnlmEmbeddingTrainerOptions>(c);
  return py;
}

} }  // namespace kaldi::rnnlm
