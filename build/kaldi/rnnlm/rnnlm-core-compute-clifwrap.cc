//////////////////////////////////////////////////////////////////////
// This file was automatically generated by CLIF to run under Python 3
// Version 0.3
//////////////////////////////////////////////////////////////////////
// source: /pykaldi/kaldi/rnnlm/rnnlm-core-compute.clif

#include <Python.h>
#include "clif/python/ptr_util.h"
#include "clif/python/optional.h"
#include "clif/python/types.h"
#include "cudamatrix/cu-matrix-clifwrap.h"
#include "nnet3/nnet-nnet-clifwrap.h"
#include "rnnlm/rnnlm-example-clifwrap.h"
#include "rnnlm/rnnlm-example-utils-clifwrap.h"
#include "rnnlm-core-compute-clifwrap.h"
#include "clif/python/stltypes.h"
#include "clif/python/slots.h"

namespace __rnnlm__core__compute_clifwrap {
using namespace clif;

#define _0 py::postconv::PASS
#define _1 UnicodeFromBytes
#define _2 UnicodeFromBytes


namespace pyRnnlmCoreComputer {

struct wrapper {
  PyObject_HEAD
  ::clif::Instance<::kaldi::rnnlm::RnnlmCoreComputer> cpp;
};
static ::kaldi::rnnlm::RnnlmCoreComputer* ThisPtr(PyObject*);

// __init__(nnet:Nnet)
static PyObject* wrapRnnlmCoreComputer_as___init__(PyObject* self, PyObject* args, PyObject* kw) {
  PyObject* a[1];
  char* names[] = {
      C("nnet"),
      nullptr
  };
  if (!PyArg_ParseTupleAndKeywords(args, kw, "O:__init__", names, &a[0])) return nullptr;
  ::kaldi::nnet3::Nnet* arg1;
  if (!Clif_PyObjAs(a[0], &arg1)) return ArgError("__init__", names[0], "::kaldi::nnet3::Nnet", a[0]);
  // Call actual C++ method.
  Py_INCREF(args);
  Py_XINCREF(kw);
  PyThreadState* _save;
  Py_UNBLOCK_THREADS
  PyObject* err_type = nullptr;
  string err_msg{"C++ exception"};
  try {
    reinterpret_cast<wrapper*>(self)->cpp = ::clif::MakeShared<::kaldi::rnnlm::RnnlmCoreComputer>(*arg1);
  } catch(const std::exception& e) {
    err_type = PyExc_RuntimeError;
    err_msg += string(": ") + e.what();
  } catch (...) {
    err_type = PyExc_RuntimeError;
  }
  Py_BLOCK_THREADS
  Py_DECREF(args);
  Py_XDECREF(kw);
  if (err_type) {
    PyErr_SetString(err_type, err_msg.c_str());
    return nullptr;
  }
  Py_RETURN_NONE;
}

// compute(minibatch:RnnlmExample, derived:RnnlmExampleDerived, word_embedding:CuMatrixBase, word_embedding_deriv:CuMatrixBase=default) -> (objf:float, weight:float)
static PyObject* wrapCompute_as_compute(PyObject* self, PyObject* args, PyObject* kw) {
  PyObject* a[4]{};
  char* names[] = {
      C("minibatch"),
      C("derived"),
      C("word_embedding"),
      C("word_embedding_deriv"),
      nullptr
  };
  if (!PyArg_ParseTupleAndKeywords(args, kw, "OOO|O:compute", names, &a[0], &a[1], &a[2], &a[3])) return nullptr;
  ::kaldi::rnnlm::RnnlmExample* arg1;
  if (!Clif_PyObjAs(a[0], &arg1)) return ArgError("compute", names[0], "::kaldi::rnnlm::RnnlmExample", a[0]);
  ::kaldi::rnnlm::RnnlmExampleDerived* arg2;
  if (!Clif_PyObjAs(a[1], &arg2)) return ArgError("compute", names[1], "::kaldi::rnnlm::RnnlmExampleDerived", a[1]);
  ::kaldi::CuMatrixBase<float>* arg3;
  if (!Clif_PyObjAs(a[2], &arg3)) return ArgError("compute", names[2], "::kaldi::CuMatrixBase<float>", a[2]);
  ::kaldi::CuMatrixBase<float> * arg4;
  if (!a[3]) arg4 = (::kaldi::CuMatrixBase<float> *)nullptr;
  else if (!Clif_PyObjAs(a[3], &arg4)) return ArgError("compute", names[3], "::kaldi::CuMatrixBase<float> *", a[3]);
  float ret1{};
  // Call actual C++ method.
  ::kaldi::rnnlm::RnnlmCoreComputer* c = ThisPtr(self);
  if (!c) return nullptr;
  Py_INCREF(args);
  Py_XINCREF(kw);
  PyThreadState* _save;
  Py_UNBLOCK_THREADS
  float ret0;
  PyObject* err_type = nullptr;
  string err_msg{"C++ exception"};
  try {
    ret0 = c->Compute(*arg1, *arg2, *arg3, arg4, &ret1);
  } catch(const std::exception& e) {
    err_type = PyExc_RuntimeError;
    err_msg += string(": ") + e.what();
  } catch (...) {
    err_type = PyExc_RuntimeError;
  }
  Py_BLOCK_THREADS
  Py_DECREF(args);
  Py_XDECREF(kw);
  if (err_type) {
    PyErr_SetString(err_type, err_msg.c_str());
    return nullptr;
  }
  // Convert return values to Python.
  PyObject* p, * result_tuple = PyTuple_New(2);
  if (result_tuple == nullptr) return nullptr;
  if ((p=Clif_PyObjFrom(std::move(ret0), {})) == nullptr) {
    Py_DECREF(result_tuple);
    return nullptr;
  }
  PyTuple_SET_ITEM(result_tuple, 0, p);
  if ((p=Clif_PyObjFrom(std::move(ret1), {})) == nullptr) {
    Py_DECREF(result_tuple);
    return nullptr;
  }
  PyTuple_SET_ITEM(result_tuple, 1, p);
  return result_tuple;
}

static PyMethodDef Methods[] = {
  {C("__init__"), (PyCFunction)wrapRnnlmCoreComputer_as___init__, METH_VARARGS | METH_KEYWORDS, C("__init__(nnet:Nnet)\n  Calls C++ function\n  void ::kaldi::rnnlm::RnnlmCoreComputer::RnnlmCoreComputer(::kaldi::nnet3::Nnet)")},
  {C("compute"), (PyCFunction)wrapCompute_as_compute, METH_VARARGS | METH_KEYWORDS, C("compute(minibatch:RnnlmExample, derived:RnnlmExampleDerived, word_embedding:CuMatrixBase, word_embedding_deriv:CuMatrixBase=default) -> (objf:float, weight:float)\n\nComputes the objective on one minibatch.\n\nIf `word_embedding_deriv` is provided, it also computes derivatives\nw.r.t. the embedding.\n\nArgs:\n  minibatch (RnnlmExample): The RNNLM minibatch to evaluate, containing\n    a number of parallel word sequences.  It will not necessarily\n    contain words with the 'original' numbering, it will in most\n    circumstances contain just the ones we used; see\n    :meth:`renumber_rnnlm_example`.\n  derived (RnnlmExampleDerived): Derived quantities of the minibatch,\n    pre-computed by calling :meth:`get_rnnlm_example_derived` with\n    suitable arguments.\n  word_embedding (CuMatrixBase): The matrix giving the embedding of\n    words, of dimension `minibatch.vocab_size` by the embedding\n    dimension. The numbering of the words does not have to be the\n    'real' numbering of words, it can consist of words renumbered by\n    :meth:`renumber_rnnlm_example`; it just has to be consistent with\n    the word-ids present in 'minibatch'.\n  word_embedding_deriv (CuMatrixBase): If not None, the derivative of\n    the objective function w.r.t. the word embedding will be *added* to\n    this location; it must have the same dimension as 'word_embedding'.\n\nReturns:\n  * **objf** -- The total objective function for this minibatch; divide\n    this by `weight` to normalize it (i.e. get the average log-prob per\n    word).\n  * **weight** -- The total weight of the words in the minibatch. This\n    is just the sum of `minibatch.output_weights`.")},
  {}
};

// RnnlmCoreComputer __init__
static int _ctor(PyObject* self, PyObject* args, PyObject* kw);

// RnnlmCoreComputer __new__
static PyObject* _new(PyTypeObject* type, Py_ssize_t nitems);

// RnnlmCoreComputer __del__
static void _dtor(PyObject* self) {
  Py_BEGIN_ALLOW_THREADS
  reinterpret_cast<wrapper*>(self)->cpp.Destruct();
  Py_END_ALLOW_THREADS
  Py_TYPE(self)->tp_free(self);
}
static void _del(void* self) {
  delete reinterpret_cast<wrapper*>(self);
}

PyTypeObject wrapper_Type = {
  PyVarObject_HEAD_INIT(&PyType_Type, 0)
  "_rnnlm_core_compute.RnnlmCoreComputer", // tp_name
  sizeof(wrapper),                     // tp_basicsize
  0,                                   // tp_itemsize
  _dtor,                               // tp_dealloc
  nullptr,                             // tp_print
  nullptr,                             // tp_getattr
  nullptr,                             // tp_setattr
  nullptr,                             // tp_compare
  nullptr,                             // tp_repr
  nullptr,                             // tp_as_number
  nullptr,                             // tp_as_sequence
  nullptr,                             // tp_as_mapping
  nullptr,                             // tp_hash
  nullptr,                             // tp_call
  nullptr,                             // tp_str
  nullptr,                             // tp_getattro
  nullptr,                             // tp_setattro
  nullptr,                             // tp_as_buffer
  Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE, // tp_flags
  "Core RNNLM computer.\n\nThis class has a similar interface to `RnnlmCoreTrainer`, but it doesn't\nactually train the RNNLM; it's for computing likelihoods and (optionally)\nderivatives w.r.t. the embedding, in situations where you are not\ntraining the core part of the RNNLM. It reads egs-- it's not for\nrescoring lattices and similar purposes.\n\nArgs:\n  nnet (Nnet): The neural network that is to be used to evaluate\n    likelihoods (and possibly derivatives).", // tp_doc
  nullptr,                             // tp_traverse
  nullptr,                             // tp_clear
  nullptr,                             // tp_richcompare
  0,                                   // tp_weaklistoffset
  nullptr,                             // tp_iter
  nullptr,                             // tp_iternext
  Methods,                             // tp_methods
  nullptr,                             // tp_members
  nullptr,                             // tp_getset
  nullptr,                             // tp_base
  nullptr,                             // tp_dict
  nullptr,                             // tp_descr_get
  nullptr,                             // tp_descr_set
  0,                                   // tp_dictoffset
  _ctor,                               // tp_init
  _new,                                // tp_alloc
  PyType_GenericNew,                   // tp_new
  _del,                                // tp_free
  nullptr,                             // tp_is_gc
  nullptr,                             // tp_bases
  nullptr,                             // tp_mro
  nullptr,                             // tp_cache
  nullptr,                             // tp_subclasses
  nullptr,                             // tp_weaklist
  nullptr,                             // tp_del
  0,                                   // tp_version_tag
};

static int _ctor(PyObject* self, PyObject* args, PyObject* kw) {
  PyObject* init = wrapRnnlmCoreComputer_as___init__(self, args, kw);
  Py_XDECREF(init);
  return init? 0: -1;
}

static PyObject* _new(PyTypeObject* type, Py_ssize_t nitems) {
  assert(nitems == 0);
  PyObject* self = reinterpret_cast<PyObject*>(new wrapper);
  return PyObject_Init(self, &wrapper_Type);
}

static ::kaldi::rnnlm::RnnlmCoreComputer* ThisPtr(PyObject* py) {
  if (Py_TYPE(py) == &wrapper_Type) {
    return ::clif::python::Get(reinterpret_cast<wrapper*>(py)->cpp);
  }
  PyObject* base = PyObject_CallMethod(py, C("as_kaldi_rnnlm_RnnlmCoreComputer"), nullptr);
  if (base) {
    if (PyCapsule_CheckExact(base)) {
      void* p = PyCapsule_GetPointer(base, C("::kaldi::rnnlm::RnnlmCoreComputer"));
      if (!PyErr_Occurred()) {
        ::kaldi::rnnlm::RnnlmCoreComputer* c = static_cast<::kaldi::rnnlm::RnnlmCoreComputer*>(p);
        Py_DECREF(base);
        return c;
      }
    }
    Py_DECREF(base);
  }
  if (PyObject_IsInstance(py, reinterpret_cast<PyObject*>(&wrapper_Type))) {
    if (!base) {
      PyErr_Clear();
      return ::clif::python::Get(reinterpret_cast<wrapper*>(py)->cpp);
    }
    PyErr_Format(PyExc_ValueError, "can't convert %s %s to ::kaldi::rnnlm::RnnlmCoreComputer*", ClassName(py), ClassType(py));
  } else {
    PyErr_Format(PyExc_TypeError, "expecting %s instance, got %s %s", wrapper_Type.tp_name, ClassName(py), ClassType(py));
  }
  return nullptr;
}
}  // namespace pyRnnlmCoreComputer


// Initialize module

bool Ready() {
  if (PyType_Ready(&pyRnnlmCoreComputer::wrapper_Type) < 0) return false;
  Py_INCREF(&pyRnnlmCoreComputer::wrapper_Type);  // For PyModule_AddObject to steal.
  return true;
}

static struct PyModuleDef Module = {
  PyModuleDef_HEAD_INIT,
  "_rnnlm_core_compute",  // module name
  "CLIF-generated module for rnnlm/rnnlm-core-compute.h", // module doc
  -1,  // module keeps state in global variables
  nullptr
};

PyObject* Init() {
  PyObject* module = PyModule_Create(&Module);
  if (!module) return nullptr;
  if (PyObject* m = PyImport_ImportModule("_cu_matrix")) Py_DECREF(m);
  else goto err;
  if (PyObject* m = PyImport_ImportModule("_nnet_nnet")) Py_DECREF(m);
  else goto err;
  if (PyObject* m = PyImport_ImportModule("_rnnlm_example")) Py_DECREF(m);
  else goto err;
  if (PyObject* m = PyImport_ImportModule("_rnnlm_example_utils")) Py_DECREF(m);
  else goto err;
  PyEval_InitThreads();
  if (PyModule_AddObject(module, "RnnlmCoreComputer", reinterpret_cast<PyObject*>(&pyRnnlmCoreComputer::wrapper_Type)) < 0) goto err;
  return module;
err:
  Py_DECREF(module);
  return nullptr;
}

}  // namespace __rnnlm__core__compute_clifwrap

namespace kaldi { namespace rnnlm {
using namespace ::clif;
using ::clif::Clif_PyObjAs;
using ::clif::Clif_PyObjFrom;

// RnnlmCoreComputer to/from ::kaldi::rnnlm::RnnlmCoreComputer conversion

bool Clif_PyObjAs(PyObject* py, ::kaldi::rnnlm::RnnlmCoreComputer** c) {
  assert(c != nullptr);
  if (Py_None == py) {
    *c = nullptr;
    return true;
  }
  ::kaldi::rnnlm::RnnlmCoreComputer* cpp = __rnnlm__core__compute_clifwrap::pyRnnlmCoreComputer::ThisPtr(py);
  if (cpp == nullptr) return false;
  *c = cpp;
  return true;
}

bool Clif_PyObjAs(PyObject* py, std::shared_ptr<::kaldi::rnnlm::RnnlmCoreComputer>* c) {
  assert(c != nullptr);
  ::kaldi::rnnlm::RnnlmCoreComputer* cpp = __rnnlm__core__compute_clifwrap::pyRnnlmCoreComputer::ThisPtr(py);
  if (cpp == nullptr) return false;
  *c = ::clif::MakeStdShared(reinterpret_cast<__rnnlm__core__compute_clifwrap::pyRnnlmCoreComputer::wrapper*>(py)->cpp, cpp);
  return true;
}

bool Clif_PyObjAs(PyObject* py, std::unique_ptr<::kaldi::rnnlm::RnnlmCoreComputer>* c) {
  assert(c != nullptr);
  ::kaldi::rnnlm::RnnlmCoreComputer* cpp = __rnnlm__core__compute_clifwrap::pyRnnlmCoreComputer::ThisPtr(py);
  if (cpp == nullptr) return false;
  if (!reinterpret_cast<__rnnlm__core__compute_clifwrap::pyRnnlmCoreComputer::wrapper*>(py)->cpp.Detach()) {
    PyErr_SetString(PyExc_ValueError, "Cannot convert RnnlmCoreComputer instance to std::unique_ptr.");
    return false;
  }
  c->reset(cpp);
  return true;
}

PyObject* Clif_PyObjFrom(::kaldi::rnnlm::RnnlmCoreComputer* c, py::PostConv unused) {
  if (c == nullptr) Py_RETURN_NONE;
  PyObject* py = PyType_GenericNew(&__rnnlm__core__compute_clifwrap::pyRnnlmCoreComputer::wrapper_Type, NULL, NULL);
  reinterpret_cast<__rnnlm__core__compute_clifwrap::pyRnnlmCoreComputer::wrapper*>(py)->cpp = ::clif::Instance<::kaldi::rnnlm::RnnlmCoreComputer>(c, ::clif::UnOwnedResource());
  return py;
}

PyObject* Clif_PyObjFrom(std::shared_ptr<::kaldi::rnnlm::RnnlmCoreComputer> c, py::PostConv unused) {
  if (c == nullptr) Py_RETURN_NONE;
  PyObject* py = PyType_GenericNew(&__rnnlm__core__compute_clifwrap::pyRnnlmCoreComputer::wrapper_Type, NULL, NULL);
  reinterpret_cast<__rnnlm__core__compute_clifwrap::pyRnnlmCoreComputer::wrapper*>(py)->cpp = ::clif::Instance<::kaldi::rnnlm::RnnlmCoreComputer>(c);
  return py;
}

PyObject* Clif_PyObjFrom(std::unique_ptr<::kaldi::rnnlm::RnnlmCoreComputer> c, py::PostConv unused) {
  if (c == nullptr) Py_RETURN_NONE;
  PyObject* py = PyType_GenericNew(&__rnnlm__core__compute_clifwrap::pyRnnlmCoreComputer::wrapper_Type, NULL, NULL);
  reinterpret_cast<__rnnlm__core__compute_clifwrap::pyRnnlmCoreComputer::wrapper*>(py)->cpp = ::clif::Instance<::kaldi::rnnlm::RnnlmCoreComputer>(std::move(c));
  return py;
}

} }  // namespace kaldi::rnnlm
